---
title: "Baseball"
output: html_document
date: "2025-05-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Limitations

Due to the size of the Salary database, I was only able to look at the years from 1985-2016.


## Exploratory Data Analysis

After downloading the tidy packages and my baseball data set, the first thing to do is define the eras of baseball. Since the salary database only goes back to 1985, the eras were divided into 15 year windows from 1985-2000 and 2001-2016. Since creating each dataset takes multiple steps, the 1985-2000 called 'old' will be defined first.

```{r}
library(Lahman)
library(tidyverse)
library(tidymodels)

salaries <- Salaries |>
  dplyr::select(playerID, yearID, teamID, salary)
peopleInfo <- People |>
  dplyr::select(playerID, birthYear, birthMonth, nameLast,
         nameFirst, bats)
batting <- battingStats() |>
  left_join(salaries, 
            by =c("playerID", "yearID", "teamID")) |>
  left_join(peopleInfo, by = "playerID") |>
  mutate(age = yearID - birthYear - 
           1L *(birthMonth >= 10)) |>
  arrange(playerID, yearID, stint)

batting_older <- batting |> filter(G >= 100, AB >= 200, 2001 > yearID, yearID > 1984) |>
  mutate(lgID = factor(lgID, levels = c("AL", "NL"))) # fix the defunct league issue

batting_older <- batting_older |>
  mutate(team_payroll = case_when(
    teamID %in% c("ANA", "BOS", "CAL", "LAN", "NYA", "NYN", "PHI", "LAA") ~ "top",
    teamID %in% c("CHA", "CHN", "DET", "SEA", "SFN", "SLN") ~ "upper",
    teamID %in% c("ATL", "CIN", "HOU", "MON", "TEX", "TOR") ~ "middle",
    teamID %in% c("ARI", "BAL", "COL", "MIN", "ML4", "SDN") ~ "lower",
    teamID %in% c("CLE", "FLO", "KCA", "OAK", "PIT", "TBA") ~ "bottom",
    TRUE ~ NA_character_  # Optional: catch all others as NA
  ))

```

The data set will need additional variables, so it was modified briefly to include these changes.

```{r}

bat_old_proto <- batting_older |>
  transmute(
    Name = paste(nameFirst, nameLast),
    Year = yearID,
    team_payroll = team_payroll,
    PA = PA,
    BA = BA,
    HR = HR,
    R = R,
    RBI = RBI,
    age=age,
    salary=salary,
    SlugPct = SlugPct,
    OBP = OBP, # these 3 are already rate stats
    BBRate = BB/PA, # walks per time at the plate
    SORate = SO/PA, # strikeouts per time at the plate
    DoubleRate = X2B/PA,
    TripleRate = X3B/PA,
    HRRate = HR/PA,
    RRate = R/PA,
    RBIRate = RBI/PA,
    SBRate = SB/PA,
    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN
    bats,
    lgID # just to have some categorical variables here
  )
```

Due to the nature of how statistics are calculated in baseball, a HR when successfully achieved in baseball also counts as an RBI and R. Due to this overlapping nature of the statistic, HR, R, and RBI will be combined into a single variable, which will be called PC_Score using Principal Component Analysis.

Using the recipe function, we will first add the desired variables HR, R, and RBI.

```{r}
pca_recipe <- recipe(
  ~ Name + Year + HR + RBI + R, data = bat_old_proto
) |>
## ~ . indicates to use all variables in the dataset as predictors
  update_role(c(Name, Year), new_role = "id") |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_pca(all_predictors(), num_comp = 10)
```

Then, the recipe is prepped.

```{r}
pca_prep <- pca_recipe |>
  prep()
pca_prep
```
Then, the recipe is baked.

```{r}
pca_baked <- pca_prep |>
  bake(new_data = NULL)
pca_baked

```

Lastly using the tidy function, the PC values are loaded.

```{r}
pca_tidy <- tidy(pca_prep, 3, type = "coef") # tidy step 3 - the PCA step
head(pca_tidy, 20)
```
The formula is then derived for PC1, which will be changed in name to PC_Score.

```{r}
pca_loadings <- pca_tidy |>
  pivot_wider(names_from = "component",
              values_from = "value") |>
  dplyr::select(!id)
arrange(pca_loadings, desc(abs(PC1)))
arrange(pca_loadings, desc(abs(PC2)))
```

Then it is added to the data set and the values are posted below.

```{r}
bat_old_proto <- bat_old_proto |> 
  merge(pca_baked, 
            by =c("Name", "Year"))

```


```{r}
bat_old_proto <- bat_old_proto |>
  transmute(
    Name = Name,
    Year = Year,
    team_payroll = team_payroll,
    PA = PA,
    BA = BA,
    HR = HR,
    age=age,
    salary=salary,
    R = R,
    RBI = RBI,
    SlugPct = SlugPct,
    OBP = OBP, # these 3 are already rate stats
    BBRate = BBRate, # walks per time at the plate
    SORate = SORate, # strikeouts per time at the plate
    DoubleRate = DoubleRate,
    TripleRate = TripleRate,
    HRRate = HRRate,
    RRate = RRate,
    RBIRate = RBIRate,
    SBRate = SBRate,
    SBSuccessRate = SBSuccessRate,
    PC_score=PC1,
    PC1=PC1,
    PC2=PC2,
    PC3=PC3,
    OPS=OBP + SlugPct,
    # to avoid NaN
    bats,
    lgID # just to have some categorical variables here
  )
```

Now that PC_Score is added to the data set, adding the salary variable will come next. Due to inflation, the salary variable will be standardized into a Salary_ZScore.

```{r}
bat_old <- bat_old_proto |>
  group_by(Year) |>
  mutate(Salary_ZScore = scale(salary) |> as.vector()) |>
  ungroup()
```

Then repeating this process, the second data set for players from 2001-2016 are now included into a data set called 'new'.

```{r}
library(Lahman)
library(tidyverse)
library(tidymodels)

salaries <- Salaries %>%
  dplyr::select(playerID, yearID, teamID, salary)
peopleInfo <- People %>%
  dplyr::select(playerID, birthYear, birthMonth, nameLast,
         nameFirst, bats)
batting <- battingStats() %>% 
  left_join(salaries,
            by =c("playerID", "yearID", "teamID")) %>%
  left_join(peopleInfo, by = "playerID") %>%
  mutate(age = yearID - birthYear - 
           1L *(birthMonth >= 10)) %>%
  arrange(playerID, yearID, stint)

batting_new <- batting %>% filter(G >= 100, AB >= 200, 2017 > yearID, yearID > 2000) %>%
  mutate(lgID = factor(lgID, levels = c("AL", "NL"))) # fix the defunct league issue

batting_new <- batting_new |>
  mutate(team_payroll = case_when(
    teamID %in% c("ANA", "BOS", "CAL", "LAN", "NYA", "NYN", "PHI", "LAA") ~ "top",
    teamID %in% c("CHA", "CHN", "DET", "SEA", "SFN", "SLN") ~ "upper",
    teamID %in% c("ATL", "CIN", "HOU", "MON", "TEX", "TOR") ~ "middle",
    teamID %in% c("ARI", "BAL", "COL", "MIN", "ML4", "SDN") ~ "lower",
    teamID %in% c("CLE", "FLO", "KCA", "OAK", "PIT", "TBA") ~ "bottom",
    TRUE ~ NA_character_  # Optional: catch all others as NA
  ))

```

Modifying the variables.

```{r}
bat_new_proto <- batting_new |>
  transmute(
    Name = paste(nameFirst, nameLast),
    Year = yearID,
    team_payroll = team_payroll,
    PA = PA,
    BA = BA,
    HR = HR,
    R = R,
    RBI = RBI,
    age=age,
    salary=salary,
    SlugPct = SlugPct,
    OBP = OBP, # these 3 are already rate stats
    BBRate = BB/PA, # walks per time at the plate
    SORate = SO/PA, # strikeouts per time at the plate
    DoubleRate = X2B/PA,
    TripleRate = X3B/PA,
    HRRate = HR/PA,
    RRate = R/PA,
    RBIRate = RBI/PA,
    SBRate = SB/PA,
    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN
    bats,
    lgID # just to have some categorical variables here
  )
```

Creating the variable for HR/RBI/R called PC_Score.

```{r}
pca_recipe_new <- recipe(
  ~ Name + Year + HR + RBI + R, data = bat_new_proto
) |>
## ~ . indicates to use all variables in the dataset as predictors
  update_role(c(Name, Year), new_role = "id") |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_pca(all_predictors(), num_comp = 10)
```

Prepping and then baking the recipe.

```{r}
pca_prep_new <- pca_recipe_new |>
  prep()
pca_prep_new

pca_baked_new <- pca_prep_new |>
  bake(new_data = NULL)
pca_baked_new

bat_new_proto <- bat_new_proto |> 
  merge(pca_baked_new, 
            by =c("Name", "Year"))
```

Adding the variable to the dataset.

```{r}
bat_new_proto <- bat_new_proto |>
  transmute(
    Name = Name,
    Year = Year,
    team_payroll = team_payroll,
    PA = PA,
    BA = BA,
    HR = HR,
    age=age,
    salary=salary,
    R = R,
    RBI = RBI,
    SlugPct = SlugPct,
    OBP = OBP, # these 3 are already rate stats
    BBRate = BBRate, # walks per time at the plate
    SORate = SORate, # strikeouts per time at the plate
    DoubleRate = DoubleRate,
    TripleRate = TripleRate,
    HRRate = HRRate,
    RRate = RRate,
    RBIRate = RBIRate,
    SBRate = SBRate,
    SBSuccessRate = SBSuccessRate,
    PC_score=PC1,
    PC1=PC1,
    PC2=PC2,
    PC3=PC3,
    OPS=OBP + SlugPct,
    # to avoid NaN
    bats,
    lgID # just to have some categorical variables here
  )
```

Standardizing salary.

```{r}
bat_new <- bat_new_proto |>
  group_by(Year) |>
  mutate(Salary_ZScore = scale(salary) |> as.vector()) |>
  ungroup()

pca_tidy_new <- tidy(pca_prep_new, 3, type = "coef") # tidy step 3 - the PCA step
head(pca_tidy_new, 20)

pca_loadings_new <- pca_tidy_new |>
  pivot_wider(names_from = "component",
              values_from = "value") |>
  dplyr::select(!id)
arrange(pca_loadings_new, desc(abs(PC1)))
arrange(pca_loadings_new, desc(abs(PC2)))
```

Lastly, any N/A values in the variable Salary_ZScore will be dropped before continuing.

```{r}
bat_old <- bat_old |>
  drop_na(Salary_ZScore)

bat_new <- bat_new |>
  drop_na(Salary_ZScore)

```

One question that I wanted to explore before continuing was "Are the statistics different between the two group i.e. could salary be related to one of the era's performing better than the other."

```{r}
percentiles_BA_old<-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))

percentiles_BA_old
```

```{r}
percentile_BA_new<-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))

percentile_BA_new
```

```{r}
percentiles_PA_old<-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_PA_old
```

```{r}
percentile_PA_new<-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))

percentile_PA_new
```

```{r}
percentiles_PC_old<-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_PC_old
```

```{r}
percentiles_PC_new<-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_PC_new
```

```{r}
percentiles_SORate_old<-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_SORate_old
```

```{r}
percentiles_SORate_new<-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_SORate_new
```

```{r}
percentiles_OPS_old<-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_OPS_old
```

```{r}
percentiles_OPS_new<-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_OPS_new
```

```{r}
percentiles_age_old<-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_age_old
```

```{r}
percentiles_age_new<-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,
                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) 

percentiles_age_new
```

3 metrics that were especially looked at when comparing these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.

Old BA 25%: 0.252, 50%: 0.271, 75%: 0.292
New BA 25%: 0.251, 50%: 0.270, 75%: 0.289

Old OPS 25%: 0.694, 50%: 0.760, 75%: 0.832
New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831

Old PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01
New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02

The eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being based on statistics but then pondering the top 10%, 5%, 1% of players, these values were also noted to see if there is anything interesting with the outliers.

Old BA 90%: 0.311, 95%: 0.323, 99%: 0.349
New BA 90%: 0.316, 95%: 0.318, 99%: 0.338

Old OPS 90%: 0.904, 95%: 0.957, 99%: 1.06
New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04

Old PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21
New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11

These numbers are very consistent across the board with all the percentiles that were chosen and there is no significant distinction between them just based on the eye test i.e. better statistics can be ruled out as a possible cause of any pay disparity.

Lastly, the correlation matrix was created to see if there are any statistics that are worth looking at for similarity to combine into one variable similar to what was done to PC_Score. This led to finding a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that PC_score is already a heavy hitter stat, OPS and BA were not combined into a single variable. 

For example, Ichiro, a dominant contact hitter with a league leading 0.350 BA, would be over looked if BA were omitted because as a power hitter, he was not nearly as powerful as the other players and he would get overlooked in the database if the league leading contact part of his game were omitted.

```{r}
library(corrplot)
FLINGO<-bat_new[,c(4,5,7,14,22,26,29)]
ZINGO<-bat_old[,c(4,5,7,14,22,26,29)]

BLINGO<-cor(ZINGO)
BINGO<-cor(FLINGO)

corrplot(BINGO, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45, addCoef.col = "black")

corrplot(BLINGO, method = "circle", type = "upper",
         tl.col = "black", tl.srt = 45, addCoef.col = "black")

```

## Modeling

Now to actually look at the salary variable in a ridge regression model, first the old and new data sets will be split into 4 data sets called test_old, test_new, train_old, and train_new.

```{r}
n<-nrow(bat_old)
m<-nrow(bat_new)

indices_old<-sample(n, 0.8*floor(n), replace=F)
indices_new<-sample(m, 0.8*floor(m), replace=F)

test_old<- bat_old[-indices_old, ]
train_old<- bat_old[indices_old, ]

test_new<- bat_new[-indices_new, ]
train_new <- bat_new[indices_new, ]
```

First, the ridge model for the old data set is created below.

```{r make model using linear regression}
ridge_model_old <- linear_reg(mode = "regression", engine = "glmnet",
                          penalty = tune(), # let's tune the lambda penalty term
                          mixture = 0) # mixture = 0 specifies pure ridge regression

ridge_wflow_old <- workflow() |>
  add_model(ridge_model_old)
```

The desired variables are then added into the recipe.

```{r make recipe/ combine in workflow}
ridge_recipe_old <- recipe(
  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors
  data = train_old
) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors()) |> # don't scale the response
  step_dummy(all_nominal_predictors())

ridge_wflow_old <- ridge_wflow_old |>
  add_recipe(ridge_recipe_old)
```

Then, the recipe is cross validated and tuned.

```{r}
set.seed(1332)
batting_cv_old <- vfold_cv(train_old, v = 10)

ridge_tune_old <- tune_grid(ridge_model_old, 
                         ridge_recipe_old,
                      resamples = batting_cv_old, 
                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))
```

The desired tuning parameters are pulled from the model.

```{r tune model}
ridge_tune_old |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +
  scale_x_log10()
```

The model is then finalized.

```{r get lambda from the tuned model}
ridge_best_old <- ridge_tune_old |>
  select_by_one_std_err(
    metric = "rmse",
    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest
)
ridge_best_old

ridge_wflow_final_old <- ridge_wflow_old |>
  finalize_workflow(parameters = ridge_best_old) 

ridge_wflow_final_old <- fit(ridge_wflow_final_old, data=train_old) 
ridge_wflow_final_old
```

The predictions are made and a calibration plot is made to make sure that the model works.

```{r add lambda to workflow}
ridge_pred_check_old <- ridge_wflow_final_old |>
  fit_resamples(
    resamples = batting_cv_old,
    # save the cross-validated predictions
    control = control_resamples(save_pred = TRUE)
) |> 
  collect_predictions()

# using built-in defaults from probably
library(ggplot2)

# Assume y_actual and y_pred are your actual and predicted values
df <- data.frame(test_old, y_pred = predict(ridge_wflow_final_old, new_data = test_old))

ggplot(df, aes(x = .pred, y = Salary_ZScore)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Calibration Plot", x = "Predicted", y = "Actual") +
  theme_minimal()
```

Now, the finals pieces of the model are computed.

```{r run model with train data}
ridge_fit_old <- ridge_wflow_final_old |>
  fit(data = train_old)
ridge_fit_old
```

```{r plot lambdas}
ridge_fit_old |>
  extract_fit_engine() |>
  plot(xvar = "lambda", label = TRUE)
```

```{r get c-oefficients}
ridge_coef_old <- ridge_fit_old |>
  broom::tidy()
ridge_coef_old
```

The ridge regression model has been completed below.

```{r make predictions}
predictions_ridge_old <- ridge_fit_old |>
  broom::augment(new_data = test_old)
predictions_ridge_old |>
  dplyr::select(Name,
                Salary_ZScore,
    
    everything()
)

rmse(predictions_ridge_old, truth = Salary_ZScore, estimate = .pred)


```

```{r}
ridge_model_new <- linear_reg(mode = "regression", engine = "glmnet",
                          penalty = tune(), # let's tune the lambda penalty term
                          mixture = 0) # mixture = 0 specifies pure ridge regression

ridge_wflow_new <- workflow() |>
  add_model(ridge_model_new)
```

The same process is repeated for the 'new' model. First with the recipe.

```{r}
ridge_recipe_new <- recipe(
  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors
  data = train_new
) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |> # don't scale the response
  step_dummy(all_nominal_predictors())

ridge_wflow_new <- ridge_wflow_new |>
  add_recipe(ridge_recipe_new)
```

Then, the new model is tuned and cross validated.

```{r}
set.seed(1332)
batting_cv_new <- vfold_cv(train_new, v = 10)

ridge_tune_new <- tune_grid(ridge_model_new, 
                         ridge_recipe_new,
                      resamples = batting_cv_new, 
                      grid = grid_regular(penalty(range = c(-1, 2.5)), levels = 50))
```

The metrics are collected.

```{r}
ridge_tune_new |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +
  scale_x_log10()
```

The workflow is now updated with the new 'rmse' value.

```{r}
ridge_best_new <- ridge_tune_new |>
  select_by_one_std_err(
    metric = "rmse",
    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest
)
ridge_best_new

ridge_wflow_final_new <- ridge_wflow_new |>
  finalize_workflow(parameters = ridge_best_new) 

ridge_wflow_final_new <- fit(ridge_wflow_final_new, data=train_new) 
```

The predictions are made.

```{r}
ridge_pred_check_new <- ridge_wflow_final_new |>
  fit_resamples(
    resamples = batting_cv_new,
    # save the cross-validated predictions
    control = control_resamples(save_pred = TRUE)
) |> 
  collect_predictions()

# using built-in defaults from probably
library(ggplot2)

# Assume y_actual and y_pred are your actual and predicted values
df <- data.frame(test_new, y_pred = predict(ridge_wflow_final_new, new_data = test_new))

ggplot(df, aes(x = .pred, y = Salary_ZScore)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Calibration Plot", x = "Predicted", y = "Actual") +
  theme_minimal()
```

The training data is fit.

```{r}
ridge_fit_new <- ridge_wflow_final_new |>
  fit(data = train_new)
ridge_fit_new
```

The lambda values are plotted.

```{r}
ridge_fit_new |>
  extract_fit_engine() |>
  plot(xvar = "lambda", label = TRUE)
```

The updated co-efficients are added.

```{r}
ridge_coef_new <- ridge_fit_new |>
  broom::tidy()
ridge_coef_new
```

The predictions are made.

```{r}
predictions_ridge_new <- ridge_fit_new |>
  broom::augment(new_data = test_new)
predictions_ridge_new |>
  dplyr::select(
    Name,
    Salary_ZScore, 
    .pred,
    everything()
)

```

## Experiment

In this experiment, the test set from the 'new' era will be placed into the 'old' model and vice versa to see, which model predicts larger salary z_score values. 

predictions_ridge_old_new will be the 'old model' with 'new data'
predictions_ridge_new will be the 'new model' with the 'new data'

```{r}
predictions_ridge_old_new <- ridge_fit_old |>
  broom::augment(new_data = test_new)
predictions_ridge_old_new |>
  dplyr::select(Name,
                Salary_ZScore,
    
    everything()
)

predictions_ridge_old_new |>
  dplyr::select(
    Name,
    Salary_ZScore, 
    .pred,
    everything()
)

rmse(predictions_ridge_new, truth = Salary_ZScore, estimate = .pred)
rmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)



```

predictions_ridge_new_old will be the 'new model' with the 'old data'
predictions_ridge_old will be the 'old model' with the 'old data'

```{r}
predictions_ridge_new_old <- ridge_fit_new |>
  broom::augment(new_data = test_old)
predictions_ridge_new_old |>
  dplyr::select(
    Name,
    Salary_ZScore, 
    .pred,
    everything()
)
predictions_ridge_old |>
  dplyr::select(
    Name,
    Salary_ZScore, 
    .pred,
    everything()
)

rmse(predictions_ridge_new_old, truth = Salary_ZScore, estimate = .pred)
```

As it turns out, there is a difference between the models and the older model predicts higher salary z-scores for both models indicating that the older players are predicted to be payed more than the new players thus showing that there is a pay disparity between the older group of players during the steroid era and the newer group of players.

Now, I will check for the significance of this disparity to see if any conclusions can be drawn from this model by conducting a two-sample t-test.

First it will judge based on the 'new data'

```{r}
groupA<-predictions_ridge_old_new$.pred
groupB<-predictions_ridge_new$.pred


t.test(groupA, groupB, var.equal = FALSE)

```

Then, it will judge based on the 'old data'

```{r}
group1<-predictions_ridge_old$.pred
group2<-predictions_ridge_new_old$.pred

t.test(group1, group2, var.equal = FALSE)
```

Upon further investigation, the two sample t.test shows that there is not enough evidence to support the alternative hypothesis that there is a difference of means and thus we conclude not only that the players in the old data set and the new data set have no significant difference in stats but adjusting for inflation no significant differences in salaries as well. With p-values of .35 and .23, we can conclude that there is most likely a weak correlation between old players and higher salaries but not anything conclusive enough to overturn the null hypothesis.

## Insights

I saw that the model did hint at a possibility that there was a marginal difference even though we cannot reject the null hypothesis. That being said, there are a few takeaways from this experiment that I gathered. One is that the distributions of the batting statistics across the board for both era's are very similar at each percentile range. The players are very consistently producing a very similar distribution of HR, BA, etc year after year. Also, the salaries are very skewed for the league best players and the best players as expected would receive the highest pay.

### Limitations and Future Work

The clear limitation to this project would be the overall limited-ness of the salary database. As mentioned previously, there was not a very large data set for salaries since they only went back to the 1980s. There was also no clear distinction of the salary cap of each team during this time and no mentions of a player getting payed more because he played for the Yankees for example. Future work could include just looking at the top few teams in the game to separate this dataset even more to account for the fact that the most valuable teams will pay their players the most money.

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
