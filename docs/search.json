[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Baseball.html",
    "href": "Baseball.html",
    "title": "Baseball",
    "section": "",
    "text": "The first thing that I need to do is define my eras of baseball. Since the salary database only goes back to 1985, I divided my old and new eras into 15 year windows from 1985-2000 and 2001-2016\n\nlibrary(Lahman)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nsalaries &lt;- Salaries |&gt;\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People |&gt;\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() |&gt;\n  left_join(salaries, \n            by =c(\"playerID\", \"yearID\", \"teamID\")) |&gt;\n  left_join(peopleInfo, by = \"playerID\") |&gt;\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) |&gt;\n  arrange(playerID, yearID, stint)\n\nbatting_older &lt;- batting |&gt; filter(G &gt;= 100, AB &gt;= 200, 2001 &gt; yearID, yearID &gt; 1984) |&gt;\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_older &lt;- batting_older |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nI modified the database a little bit to include stats that were not originally included including RBI rate and OPS, but now I am going to take Runs, RBI’s, and HR’s and turn them into a single variable that I will call PC_Score to combine 3 stats that are all directly related to each other into a single variable because a HR is also counted in the stat column as a Run(R) and an RBI.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_GhzAy\n2 RBI    0.607 PC1       pca_GhzAy\n3 R      0.539 PC1       pca_GhzAy\n4 HR     0.518 PC2       pca_GhzAy\n5 RBI    0.232 PC2       pca_GhzAy\n6 R     -0.823 PC2       pca_GhzAy\n7 HR     0.625 PC3       pca_GhzAy\n8 RBI   -0.760 PC3       pca_GhzAy\n9 R      0.178 PC3       pca_GhzAy\n\n\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))\n\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow, I wanted to take the salary variable and turn it into a z-score to account for the change in salary due to inflation. i.e. I don’t want inflation to undermine what I am trying to accomplish comparing eras because an increase in nominal salary could easily just be a function of inflation of the dollar and nothing related to the skill of the player.\n\nbat_old &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_GhzAy\n2 RBI    0.607 PC1       pca_GhzAy\n3 R      0.539 PC1       pca_GhzAy\n4 HR     0.518 PC2       pca_GhzAy\n5 RBI    0.232 PC2       pca_GhzAy\n6 R     -0.823 PC2       pca_GhzAy\n7 HR     0.625 PC3       pca_GhzAy\n8 RBI   -0.760 PC3       pca_GhzAy\n9 R      0.178 PC3       pca_GhzAy\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nLooking at the top of the list and seeing the heaviest hitters Barry Bonds, Sammy Sosa, and Alex Rodriguez on the list, gave me the idea that the list made sense. I also looked for a player like Ichiro, one of the most talented contact hitters, to see how he would be evaluated. Given that he was placed in the upper-middle of the pack, it made sense that he would not be featured with the heaviest sluggers in the league.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\n\nbat_new &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_7ANZY\n2 RBI    0.604 PC1       pca_7ANZY\n3 R      0.544 PC1       pca_7ANZY\n4 HR    -0.531 PC2       pca_7ANZY\n5 RBI   -0.225 PC2       pca_7ANZY\n6 R      0.817 PC2       pca_7ANZY\n7 HR    -0.616 PC3       pca_7ANZY\n8 RBI    0.765 PC3       pca_7ANZY\n9 R     -0.189 PC3       pca_7ANZY\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nn&lt;-nrow(bat_old)\nm&lt;-nrow(bat_new)\n\nindices_old&lt;-sample(n, 0.8*floor(n), replace=F)\nindices_new&lt;-sample(m, 0.8*floor(m), replace=F)\n\ntest_old&lt;- bat_old[-indices_old, ]\ntrain_old&lt;- bat_old[indices_old, ]\n\ntest_new&lt;- bat_new[-indices_new, ]\ntrain_new &lt;- bat_new[indices_new, ]\n\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n \npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_BA_old\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.17200 0.22400 0.23500 0.24200 0.24700 0.25200 0.25600 0.26000 0.26400 0.26800 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.27100 0.27500 0.27880 0.28300 0.28700 0.29200 0.29700 0.30400 0.31100 0.32300 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.32600 0.33000 0.33600 0.34100 0.34900 0.35871 0.39400 \n\npercentiles_PA_old\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n214.00 313.00 349.00 380.00 407.60 431.00 450.00 470.00 489.00 511.00 527.00 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%    96%    97% \n547.00 568.00 587.00 606.00 625.00 641.00 660.00 677.00 696.00 701.00 709.00 \n   98%  98.5%    99%  99.5%   100% \n717.00 721.00 726.00 737.71 773.00 \n\npercentiles_PC_old\n\n         0%          5%         10%         15%         20%         25% \n-2.99231806 -2.16175833 -1.87079048 -1.61930016 -1.38543523 -1.20970247 \n        30%         35%         40%         45%         50%         55% \n-1.01192213 -0.82900679 -0.62997862 -0.44687802 -0.23470836 -0.04265409 \n        60%         65%         70%         75%         80%         85% \n 0.18084883  0.42937859  0.72775522  1.02080990  1.37207629  1.75342264 \n        90%         95%         96%         97%         98%       98.5% \n 2.18583078  2.89302182  3.13819967  3.38046443  3.73994642  3.94390479 \n        99%       99.5%        100% \n 4.21382564  4.57420554  6.45624855 \n\npercentiles_SORate_old\n\n        0%         5%        10%        15%        20%        25%        30% \n0.02599653 0.06809067 0.08259171 0.09310484 0.10099334 0.11001969 0.11730861 \n       35%        40%        45%        50%        55%        60%        65% \n0.12377639 0.12957403 0.13578392 0.14170040 0.14766230 0.15363483 0.16165850 \n       70%        75%        80%        85%        90%        95%        96% \n0.16866649 0.17689917 0.18550138 0.19829709 0.21331720 0.23486239 0.24086542 \n       97%        98%      98.5%        99%      99.5%       100% \n0.25079023 0.26288020 0.26998529 0.28194742 0.29799283 0.38765432 \n\npercentiles_OPS_old\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.45000 0.60490 0.63900 0.66000 0.67900 0.69400 0.70700 0.72100 0.73300 0.74700 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.76000 0.77200 0.78400 0.79800 0.81400 0.83200 0.85000 0.87500 0.90420 0.95710 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.97336 0.99400 1.02000 1.03213 1.05942 1.09042 1.22200 \n\npercentiles_age_old\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n   19    23    24    25    26    26    27    27    28    28    29    29    30 \n  65%   70%   75%   80%   85%   90%   95%   96%   97%   98% 98.5%   99% 99.5% \n   30    31    31    32    33    34    36    37    37    38    39    39    40 \n 100% \n   44 \n\n\n3 metrics that I especially looked at when I wanted to compared these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being base on statistics but then I wonder about just the top 10%, 5%, 1% of players to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that I chose and there is no significant distinction between them just based on the eye test, so we can rule out different offensive numbers as a possible cause of any pay disparity.\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n \npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentile_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentile_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentile_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentile_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentile_BA_new\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.15900 0.22300 0.23300 0.24000 0.24600 0.25100 0.25500 0.25900 0.26300 0.26660 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.27000 0.27300 0.27700 0.28200 0.28500 0.28900 0.29400 0.30000 0.30600 0.31800 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.32100 0.32500 0.33000 0.33400 0.33800 0.34696 0.37200 \n\npercentile_PA_new\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n215.00 311.40 358.00 389.20 418.00 438.00 456.00 478.00 500.00 518.00 538.00 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%    96%    97% \n560.00 578.00 596.00 614.00 632.00 649.00 665.00 681.00 701.00 706.00 712.00 \n   98%  98.5%    99%  99.5%   100% \n718.84 725.00 731.00 739.00 778.00 \n\npercentile_PC_new\n\n          0%           5%          10%          15%          20%          25% \n-3.159885159 -2.236617151 -1.904976234 -1.632923721 -1.389647682 -1.197464047 \n         30%          35%          40%          45%          50%          55% \n-0.986158344 -0.800025977 -0.596640550 -0.401215686 -0.217399848 -0.002404445 \n         60%          65%          70%          75%          80%          85% \n 0.234612085  0.487738521  0.736773063  1.008367577  1.327689814  1.733745438 \n         90%          95%          96%          97%          98%        98.5% \n 2.225818652  2.908677146  3.102227281  3.292356325  3.593860825  3.885859410 \n         99%        99.5%         100% \n 4.114345227  4.562982520  6.832739065 \n\npercentile_SORate_new\n\n        0%         5%        10%        15%        20%        25%        30% \n0.04245974 0.08805424 0.10183586 0.11183144 0.12169312 0.12979890 0.13868421 \n       35%        40%        45%        50%        55%        60%        65% \n0.14614294 0.15322835 0.15976331 0.16608997 0.17353108 0.18151769 0.18917430 \n       70%        75%        80%        85%        90%        95%        96% \n0.19730342 0.20579268 0.21667851 0.22910250 0.24463240 0.27045781 0.27703585 \n       97%        98%      98.5%        99%      99.5%       100% \n0.28755084 0.30055950 0.30908875 0.32068440 0.33275529 0.36565097 \n\npercentile_OPS_new\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.48800 0.62340 0.65400 0.67400 0.68900 0.70300 0.71600 0.72800 0.74100 0.75300 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.76500 0.77800 0.78800 0.80200 0.81500 0.83100 0.85100 0.87300 0.89900 0.94960 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.96168 0.97976 1.00484 1.01688 1.04092 1.08384 1.42100 \n\npercentile_age_new\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n   19    23    24    25    26    26    27    27    28    28    29    29    30 \n  65%   70%   75%   80%   85%   90%   95%   96%   97%   98% 98.5%   99% 99.5% \n   31    31    32    33    34    35    36    37    37    38    39    40    40 \n 100% \n   47 \n\n\nAnd then here are some graphs of the percentiles above to visually show also that they do not appear to be very different from each other.\n\necdf(bat_old$BA)\n\nEmpirical CDF \nCall: ecdf(bat_old$BA)\n x[1:179] =  0.172,  0.179,  0.181,  ...,  0.379,  0.394\n\nplot(ecdf(bat_old$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$age)\n\nEmpirical CDF \nCall: ecdf(bat_old$age)\n x[1:26] =     19,     20,     21,  ...,     43,     44\n\nplot(ecdf(bat_old$age))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PA)\n\nEmpirical CDF \nCall: ecdf(bat_old$PA)\n x[1:511] =    214,    219,    221,  ...,    758,    773\n\nplot(ecdf(bat_old$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_old$PC_score)\n x[1:3266] = -2.9923, -2.8918, -2.8604,  ..., 6.3425, 6.4562\n\nplot(ecdf(bat_old$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_old$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_old$OPS)\n x[1:683] =   0.45,  0.465,  0.466,  ...,  1.216,  1.222\n\nplot(ecdf(bat_old$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_old$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_old$SORate)\n x[1:2991] = 0.025997, 0.027237, 0.02812,  ..., 0.36406, 0.38765\n\nplot(ecdf(bat_old$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_old$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_old$Salary_ZScore)\n x[1:2315] = -1.2453, -1.245, -1.2447,  ..., 3.6882, 3.7975\n\nplot(ecdf(bat_old$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_new$Salary_ZScore)\n x[1:2410] = -0.95118, -0.95065, -0.94986,  ..., 5.2009, 5.2309\n\nplot(ecdf(bat_new$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$BA)\n\nEmpirical CDF \nCall: ecdf(bat_new$BA)\n x[1:176] =  0.159,  0.174,  0.178,  ...,   0.37,  0.372\n\nplot(ecdf(bat_new$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PA)\n\nEmpirical CDF \nCall: ecdf(bat_new$PA)\n x[1:518] =    215,    221,    222,  ...,    765,    778\n\nplot(ecdf(bat_new$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_new$PC_score)\n x[1:3565] = -3.1599, -3.1129, -3.1032,  ..., 6.4042, 6.8327\n\nplot(ecdf(bat_new$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_new$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_new$SORate)\n x[1:3272] = 0.04246, 0.045455, 0.046791,  ..., 0.36239, 0.36565\n\nplot(ecdf(bat_new$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_new$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_new$OPS)\n x[1:661] =  0.488,  0.521,   0.53,  ...,  1.381,  1.421\n\nplot(ecdf(bat_new$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_new$age)\n\nEmpirical CDF \nCall: ecdf(bat_new$age)\n x[1:27] =     20,     21,     22,  ...,     46,     47\n\nplot(ecdf(bat_new$age))\n\n\n\n\n\n\n\n\n2222222\n22222222\n22222222\n2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\nEXPERIMENT\nBriefly plotting the correlation to see if there are any statistics that are worth looking at for similarity to combine into one variable led me to a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that I already have a heavy hitter stat for PC_score, I decided to keep these variables separate to account for this difference. For example, Ichiro was as dominant as he was due to his league leading 0.350+ BA but if you looked at OPS, we was about 0.82, which barely cracks the top 25% of players and he is most definitely a player that was in the top 5% of players in the league and a former league MVP. For this reason, I kept the two metrics separate.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nThe scatter plots below I do not believe are particularly relevant to the salary discussion because of how much noise there are in the graphs but to show that they do not really contribute to the salary discussion, I plotted them below to at least see the visual data.\n\nplot(bat_new$BA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$age, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$BA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$SORate, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$age)\n\n\n\n\n\n\n\n\nLastly, I decided to compute a ridge model, one for the old players and one for the new players to see if there was anything that could be observed for the trends of the salaries based on these two eras.\n\nridge_model_old &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_old &lt;- workflow() |&gt;\n  add_model(ridge_model_old)\n\n\nridge_recipe_old &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_old\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_old &lt;- ridge_wflow_old |&gt;\n  add_recipe(ridge_recipe_old)\n\n\nset.seed(1332)\nbatting_cv_old &lt;- vfold_cv(train_old, v = 10)\n\nridge_tune_old &lt;- tune_grid(ridge_model_old, \n                         ridge_recipe_old,\n                      resamples = batting_cv_old, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\n\nridge_tune_old |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nridge_best_old &lt;- ridge_tune_old |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_old\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.202 Preprocessor1_Model17\n\nridge_wflow_final_old &lt;- ridge_wflow_old |&gt;\n  finalize_workflow(parameters = ridge_best_old) \n\nridge_wflow_final_old &lt;- fit(ridge_wflow_final_old, data=train_old) \nridge_wflow_final_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 483.30\n2    6  0.35 440.40\n3    6  0.38 401.20\n4    6  0.42 365.60\n5    6  0.46 333.10\n6    6  0.51 303.50\n7    6  0.56 276.60\n8    6  0.61 252.00\n9    6  0.67 229.60\n10   6  0.73 209.20\n11   6  0.80 190.60\n12   6  0.88 173.70\n13   6  0.96 158.30\n14   6  1.05 144.20\n15   6  1.15 131.40\n16   6  1.26 119.70\n17   6  1.38 109.10\n18   6  1.51  99.39\n19   6  1.66  90.56\n20   6  1.81  82.51\n21   6  1.98  75.18\n22   6  2.17  68.50\n23   6  2.37  62.42\n24   6  2.59  56.87\n25   6  2.82  51.82\n26   6  3.08  47.22\n27   6  3.36  43.02\n28   6  3.66  39.20\n29   6  3.99  35.72\n30   6  4.34  32.55\n31   6  4.72  29.65\n32   6  5.14  27.02\n33   6  5.58  24.62\n34   6  6.05  22.43\n35   6  6.56  20.44\n36   6  7.11  18.62\n37   6  7.69  16.97\n38   6  8.31  15.46\n39   6  8.97  14.09\n40   6  9.67  12.84\n41   6 10.41  11.70\n42   6 11.19  10.66\n43   6 12.01   9.71\n44   6 12.87   8.85\n45   6 13.77   8.06\n46   6 14.70   7.35\n\n...\nand 54 more lines.\n\n\n\nridge_pred_check_old &lt;- ridge_wflow_final_old |&gt;\n  fit_resamples(\n    resamples = batting_cv_old,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_old, y_pred = predict(ridge_wflow_final_old, new_data = test_old))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nridge_fit_old &lt;- ridge_wflow_final_old |&gt;\n  fit(data = train_old)\nridge_fit_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 483.30\n2    6  0.35 440.40\n3    6  0.38 401.20\n4    6  0.42 365.60\n5    6  0.46 333.10\n6    6  0.51 303.50\n7    6  0.56 276.60\n8    6  0.61 252.00\n9    6  0.67 229.60\n10   6  0.73 209.20\n11   6  0.80 190.60\n12   6  0.88 173.70\n13   6  0.96 158.30\n14   6  1.05 144.20\n15   6  1.15 131.40\n16   6  1.26 119.70\n17   6  1.38 109.10\n18   6  1.51  99.39\n19   6  1.66  90.56\n20   6  1.81  82.51\n21   6  1.98  75.18\n22   6  2.17  68.50\n23   6  2.37  62.42\n24   6  2.59  56.87\n25   6  2.82  51.82\n26   6  3.08  47.22\n27   6  3.36  43.02\n28   6  3.66  39.20\n29   6  3.99  35.72\n30   6  4.34  32.55\n31   6  4.72  29.65\n32   6  5.14  27.02\n33   6  5.58  24.62\n34   6  6.05  22.43\n35   6  6.56  20.44\n36   6  7.11  18.62\n37   6  7.69  16.97\n38   6  8.31  15.46\n39   6  8.97  14.09\n40   6  9.67  12.84\n41   6 10.41  11.70\n42   6 11.19  10.66\n43   6 12.01   9.71\n44   6 12.87   8.85\n45   6 13.77   8.06\n46   6 14.70   7.35\n\n...\nand 54 more lines.\n\n\n\nridge_fit_old |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_old &lt;- ridge_fit_old |&gt;\n  broom::tidy()\nridge_coef_old\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.00504   0.202\n2 BA          -0.0750    0.202\n3 PA           0.168     0.202\n4 PC_score     0.247     0.202\n5 OPS          0.110     0.202\n6 SORate      -0.0781    0.202\n7 age          0.398     0.202\n\n\n\npredictions_ridge_old &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_old |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 667 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adria…      -0.675   -0.562  2000 top            575 0.29     20    21 1   e6\n 2 Al Co…      -0.413    0.404  1985 upper          487 0.265    14    33 4   e5\n 3 Al Ne…      -0.725   -0.357  1990 lower          433 0.242     0    30 3.5 e5\n 4 Al Ne…      -0.849   -0.467  1992 middle         289 0.22      0    32 2.6 e5\n 5 Alan …       0.00952  0.446  1986 upper          653 0.277    21    28 5.67e5\n 6 Alan …       1.44     0.635  1990 upper          637 0.304    14    32 1.80e6\n 7 Alber…       0.103    0.683  1993 bottom         693 0.29     38    27 1.68e6\n 8 Alber…       3.47     0.894  1997 upper          701 0.274    30    31 1   e7\n 9 Alex …      -0.713   -0.174  1999 top            390 0.303     4    31 6.25e5\n10 Alex …      -0.877   -0.694  1999 bottom         591 0.277    14    22 2.01e5\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.758\n\n\n\npredictions_ridge_old_new &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_old_new |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 728 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…       -0.584  -0.0306  2013 top            448 0.238    10    32 2   e6\n 2 A. J…       -0.342  -0.158   2005 upper          497 0.257    18    28 2.25e6\n 3 A. J…        0.237   0.793   2012 upper          520 0.278    27    35 6   e6\n 4 Aaro…       -0.171   0.280   2005 bottom         565 0.243    16    32 3   e6\n 5 Aaro…       -0.846  -0.929   2016 top            361 0.217     8    26 5.74e5\n 6 Aaro…       -0.147   0.155   2010 middle         580 0.205    26    28 4   e6\n 7 Aaro…        0.0345 -0.431   2011 middle         429 0.225     6    29 5   e6\n 8 Aaro…        0.146   0.634   2012 lower          668 0.302    26    30 5.50e6\n 9 Aaro…        0.837   0.123   2014 lower          541 0.244    10    32 1.10e7\n10 Aaro…       -0.769  -0.306   2004 lower          566 0.293     6    27 3   e5\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.752\n\n\n\nridge_model_new &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_new &lt;- workflow() |&gt;\n  add_model(ridge_model_new)\n\n\nridge_recipe_new &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_new\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_new &lt;- ridge_wflow_new |&gt;\n  add_recipe(ridge_recipe_new)\n\n\nset.seed(1332)\nbatting_cv_new &lt;- vfold_cv(train_new, v = 10)\n\nridge_tune_new &lt;- tune_grid(ridge_model_new, \n                         ridge_recipe_new,\n                      resamples = batting_cv_new, \n                      grid = grid_regular(penalty(range = c(-1, 2.5)), levels = 50))\n\n\nridge_tune_new |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nridge_best_new &lt;- ridge_tune_new |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_new\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.373 Preprocessor1_Model09\n\nridge_wflow_final_new &lt;- ridge_wflow_new |&gt;\n  finalize_workflow(parameters = ridge_best_new) \n\nridge_wflow_final_new &lt;- fit(ridge_wflow_final_new, data=train_new) \n\n\nridge_pred_check_new &lt;- ridge_wflow_final_new |&gt;\n  fit_resamples(\n    resamples = batting_cv_new,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_new, y_pred = predict(ridge_wflow_final_new, new_data = test_new))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nridge_fit_new &lt;- ridge_wflow_final_new |&gt;\n  fit(data = train_new)\nridge_fit_new\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_mean()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 460.90\n2    6  0.31 419.90\n3    6  0.34 382.60\n4    6  0.37 348.60\n5    6  0.41 317.70\n6    6  0.45 289.40\n7    6  0.49 263.70\n8    6  0.54 240.30\n9    6  0.59 218.90\n10   6  0.65 199.50\n11   6  0.71 181.80\n12   6  0.78 165.60\n13   6  0.85 150.90\n14   6  0.93 137.50\n15   6  1.02 125.30\n16   6  1.12 114.20\n17   6  1.22 104.00\n18   6  1.34  94.78\n19   6  1.47  86.36\n20   6  1.60  78.68\n21   6  1.75  71.69\n22   6  1.91  65.33\n23   6  2.09  59.52\n24   6  2.28  54.23\n25   6  2.49  49.42\n26   6  2.72  45.03\n27   6  2.97  41.03\n28   6  3.23  37.38\n29   6  3.52  34.06\n30   6  3.83  31.03\n31   6  4.17  28.28\n32   6  4.53  25.77\n33   6  4.92  23.48\n34   6  5.34  21.39\n35   6  5.79  19.49\n36   6  6.27  17.76\n37   6  6.78  16.18\n38   6  7.32  14.74\n39   6  7.90  13.43\n40   6  8.52  12.24\n41   6  9.17  11.15\n42   6  9.85  10.16\n43   6 10.57   9.26\n44   6 11.32   8.44\n45   6 12.11   7.69\n46   6 12.93   7.00\n\n...\nand 54 more lines.\n\n\n\nridge_fit_new |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_new &lt;- ridge_fit_new |&gt;\n  broom::tidy()\nridge_coef_new\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.00345   0.373\n2 BA          -0.0145    0.373\n3 PA           0.124     0.373\n4 PC_score     0.184     0.373\n5 OPS          0.111     0.373\n6 SORate      -0.0206    0.373\n7 age          0.348     0.373\n\n\n\npredictions_ridge_new &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…       -0.584  -0.0786  2013 top            448 0.238    10    32 2   e6\n 2 A. J…       -0.342  -0.210   2005 upper          497 0.257    18    28 2.25e6\n 3 A. J…        0.237   0.649   2012 upper          520 0.278    27    35 6   e6\n 4 Aaro…       -0.171   0.146   2005 bottom         565 0.243    16    32 3   e6\n 5 Aaro…       -0.846  -0.854   2016 top            361 0.217     8    26 5.74e5\n 6 Aaro…       -0.147  -0.0641  2010 middle         580 0.205    26    28 4   e6\n 7 Aaro…        0.0345 -0.501   2011 middle         429 0.225     6    29 5   e6\n 8 Aaro…        0.146   0.504   2012 lower          668 0.302    26    30 5.50e6\n 9 Aaro…        0.837   0.0298  2014 lower          541 0.244    10    32 1.10e7\n10 Aaro…       -0.769  -0.327   2004 lower          566 0.293     6    27 3   e5\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…       -0.584  -0.0306  2013 top            448 0.238    10    32 2   e6\n 2 A. J…       -0.342  -0.158   2005 upper          497 0.257    18    28 2.25e6\n 3 A. J…        0.237   0.793   2012 upper          520 0.278    27    35 6   e6\n 4 Aaro…       -0.171   0.280   2005 bottom         565 0.243    16    32 3   e6\n 5 Aaro…       -0.846  -0.929   2016 top            361 0.217     8    26 5.74e5\n 6 Aaro…       -0.147   0.155   2010 middle         580 0.205    26    28 4   e6\n 7 Aaro…        0.0345 -0.431   2011 middle         429 0.225     6    29 5   e6\n 8 Aaro…        0.146   0.634   2012 lower          668 0.302    26    30 5.50e6\n 9 Aaro…        0.837   0.123   2014 lower          541 0.244    10    32 1.10e7\n10 Aaro…       -0.769  -0.306   2004 lower          566 0.293     6    27 3   e5\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.769\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.752\n\n\n\npredictions_ridge_new_old &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_new_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adria…      -0.675   -0.513  2000 top            575 0.29     20    21 1   e6\n 2 Al Co…      -0.413    0.271  1985 upper          487 0.265    14    33 4   e5\n 3 Al Ne…      -0.725   -0.445  1990 lower          433 0.242     0    30 3.5 e5\n 4 Al Ne…      -0.849   -0.531  1992 middle         289 0.22      0    32 2.6 e5\n 5 Alan …       0.00952  0.253  1986 upper          653 0.277    21    28 5.67e5\n 6 Alan …       1.44     0.481  1990 upper          637 0.304    14    32 1.80e6\n 7 Alber…       0.103    0.507  1993 bottom         693 0.29     38    27 1.68e6\n 8 Alber…       3.47     0.666  1997 upper          701 0.274    30    31 1   e7\n 9 Alex …      -0.713   -0.151  1999 top            390 0.303     4    31 6.25e5\n10 Alex …      -0.877   -0.608  1999 bottom         591 0.277    14    22 2.01e5\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adria…      -0.675   -0.562  2000 top            575 0.29     20    21 1   e6\n 2 Al Co…      -0.413    0.404  1985 upper          487 0.265    14    33 4   e5\n 3 Al Ne…      -0.725   -0.357  1990 lower          433 0.242     0    30 3.5 e5\n 4 Al Ne…      -0.849   -0.467  1992 middle         289 0.22      0    32 2.6 e5\n 5 Alan …       0.00952  0.446  1986 upper          653 0.277    21    28 5.67e5\n 6 Alan …       1.44     0.635  1990 upper          637 0.304    14    32 1.80e6\n 7 Alber…       0.103    0.683  1993 bottom         693 0.29     38    27 1.68e6\n 8 Alber…       3.47     0.894  1997 upper          701 0.274    30    31 1   e7\n 9 Alex …      -0.713   -0.174  1999 top            390 0.303     4    31 6.25e5\n10 Alex …      -0.877   -0.694  1999 bottom         591 0.277    14    22 2.01e5\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.772\n\n\nLastly, what I did was compare the old model to the new model. We did this by plugging the test_old into both models to see which model predicted a higher value and we also plugged the test_new into both models to see which model predicted the higher values. As it turned out, there was a difference between the models and the older model predicted higher salary z-scores for both models indicating that the older players are predicted to be payed more than the new players thus showing that there is a significant pay disparity between the older group of players during the steroid era and the newer group of players.\nExamples of some of these numbers\nAlbert Pujoles 2002 (Old model prediction: 0.26) (New model prediction: 0.14) Albert Pujoles 2008 (Old model prediction: 0.79) (New model prediction: 0.74) Albert Pujoles 2009 (Old model prediction: 1.26) (New model prediction: 1.07) Albert Pujoles 2015 (Old model prediction: 1.29) (New model prediction: 0.99) Albert Pujoles 2016 (Old model prediction: 1.24) (New model prediction: 1.02)\nAlex Rodriguez 2003 (Old model prediction: 0.95) (New model prediction: 0.82) Alex Rodriguez 2006 (Old model prediction: 0.93) (New model prediction: 0.84) Alex Rodriguez 2008 (Old model prediction: 0.95) (New model prediction: 0.92)\n\ngroupA&lt;-predictions_ridge_old_new$.pred\ngroupB&lt;-predictions_ridge_new$.pred\n\n\nt.test(groupA, groupB, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  groupA and groupB\nt = 0.89706, df = 1412.1, p-value = 0.3698\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03012882  0.08090432\nsample estimates:\n  mean of x   mean of y \n0.026389212 0.001001462 \n\n\n\ngroup1&lt;-predictions_ridge_old$.pred\ngroup2&lt;-predictions_ridge_new_old$.pred\n\nt.test(group1, group2, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 1.2643, df = 1297.8, p-value = 0.2064\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.02177824  0.10072548\nsample estimates:\n   mean of x    mean of y \n 0.006404504 -0.033069112 \n\n\nHowever, upon further inspection of the entire dataset and not just the outliers, we see in a two sample t.test that there is not enough evidence to support the alternative hypothesis that there is a difference of means and thus we conclude not only that the players in the old dataset and the new dataset have no significant difference in stats but adjusting for inflation no significant differences in salaries as well.\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  }
]