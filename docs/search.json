[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Final Project",
    "section": "",
    "text": "Math 437"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Baseball.html",
    "href": "Baseball.html",
    "title": "Baseball",
    "section": "",
    "text": "Due to the size of the Salary database, I was only able to look at the years from 1985-2016."
  },
  {
    "objectID": "Baseball.html#exploratory-data-analysis",
    "href": "Baseball.html#exploratory-data-analysis",
    "title": "Baseball",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nAfter downloading the tidy packages and my baseball data set, the first thing to do is define the eras of baseball. Since the salary database only goes back to 1985, the eras were divided into 15 year windows from 1985-2000 and 2001-2016. Since creating each dataset takes multiple steps, the 1985-2000 called ‘old’ will be defined first.\n\nlibrary(Lahman)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nsalaries &lt;- Salaries |&gt;\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People |&gt;\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() |&gt;\n  left_join(salaries, \n            by =c(\"playerID\", \"yearID\", \"teamID\")) |&gt;\n  left_join(peopleInfo, by = \"playerID\") |&gt;\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) |&gt;\n  arrange(playerID, yearID, stint)\n\nbatting_older &lt;- batting |&gt; filter(G &gt;= 100, AB &gt;= 200, 2001 &gt; yearID, yearID &gt; 1984) |&gt;\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_older &lt;- batting_older |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\nThe data set will need additional variables, so it was modified briefly to include these changes.\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nDue to the nature of how statistics are calculated in baseball, a HR when successfully achieved in baseball also counts as an RBI and R. Due to this overlapping nature of the statistic, HR, R, and RBI will be combined into a single variable, which will be called PC_Score using Principal Component Analysis.\nUsing the recipe function, we will first add the desired variables HR, R, and RBI.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\nThen, the recipe is prepped.\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\nThen, the recipe is baked.\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\nLastly using the tidy function, the PC values are loaded.\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_dHI7E\n2 RBI    0.607 PC1       pca_dHI7E\n3 R      0.539 PC1       pca_dHI7E\n4 HR     0.518 PC2       pca_dHI7E\n5 RBI    0.232 PC2       pca_dHI7E\n6 R     -0.823 PC2       pca_dHI7E\n7 HR     0.625 PC3       pca_dHI7E\n8 RBI   -0.760 PC3       pca_dHI7E\n9 R      0.178 PC3       pca_dHI7E\n\n\nThe formula is then derived for PC1, which will be changed in name to PC_Score.\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\nThen it is added to the data set and the values are posted below.\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))\n\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow that PC_Score is added to the data set, adding the salary variable will come next. Due to inflation, the salary variable will be standardized into a Salary_ZScore.\n\nbat_old &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\nThen repeating this process, the second data set for players from 2001-2016 are now included into a data set called ‘new’.\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\nModifying the variables.\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nCreating the variable for HR/RBI/R called PC_Score.\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\nPrepping and then baking the recipe.\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nAdding the variable to the dataset.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nStandardizing salary.\n\nbat_new &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_l0LcF\n2 RBI    0.604 PC1       pca_l0LcF\n3 R      0.544 PC1       pca_l0LcF\n4 HR    -0.531 PC2       pca_l0LcF\n5 RBI   -0.225 PC2       pca_l0LcF\n6 R      0.817 PC2       pca_l0LcF\n7 HR    -0.616 PC3       pca_l0LcF\n8 RBI    0.765 PC3       pca_l0LcF\n9 R     -0.189 PC3       pca_l0LcF\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\nLastly, any N/A values in the variable Salary_ZScore will be dropped before continuing.\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nOne question that I wanted to explore before continuing was “Are the statistics different between the two group i.e. could salary be related to one of the era’s performing better than the other.”\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\npercentiles_BA_old\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.17200 0.22400 0.23500 0.24200 0.24700 0.25200 0.25600 0.26000 0.26400 0.26800 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.27100 0.27500 0.27880 0.28300 0.28700 0.29200 0.29700 0.30400 0.31100 0.32300 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.32600 0.33000 0.33600 0.34100 0.34900 0.35871 0.39400 \n\n\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\npercentile_BA_new\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.15900 0.22300 0.23300 0.24000 0.24600 0.25100 0.25500 0.25900 0.26300 0.26660 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.27000 0.27300 0.27700 0.28200 0.28500 0.28900 0.29400 0.30000 0.30600 0.31800 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.32100 0.32500 0.33000 0.33400 0.33800 0.34696 0.37200 \n\n\n\npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PA_old\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n214.00 313.00 349.00 380.00 407.60 431.00 450.00 470.00 489.00 511.00 527.00 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%    96%    97% \n547.00 568.00 587.00 606.00 625.00 641.00 660.00 677.00 696.00 701.00 709.00 \n   98%  98.5%    99%  99.5%   100% \n717.00 721.00 726.00 737.71 773.00 \n\n\n\npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\npercentile_PA_new\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n215.00 311.40 358.00 389.20 418.00 438.00 456.00 478.00 500.00 518.00 538.00 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%    96%    97% \n560.00 578.00 596.00 614.00 632.00 649.00 665.00 681.00 701.00 706.00 712.00 \n   98%  98.5%    99%  99.5%   100% \n718.84 725.00 731.00 739.00 778.00 \n\n\n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PC_old\n\n         0%          5%         10%         15%         20%         25% \n-2.99231806 -2.16175833 -1.87079048 -1.61930016 -1.38543523 -1.20970247 \n        30%         35%         40%         45%         50%         55% \n-1.01192213 -0.82900679 -0.62997862 -0.44687802 -0.23470836 -0.04265409 \n        60%         65%         70%         75%         80%         85% \n 0.18084883  0.42937859  0.72775522  1.02080990  1.37207629  1.75342264 \n        90%         95%         96%         97%         98%       98.5% \n 2.18583078  2.89302182  3.13819967  3.38046443  3.73994642  3.94390479 \n        99%       99.5%        100% \n 4.21382564  4.57420554  6.45624855 \n\n\n\npercentiles_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PC_new\n\n          0%           5%          10%          15%          20%          25% \n-3.159885159 -2.236617151 -1.904976234 -1.632923721 -1.389647682 -1.197464047 \n         30%          35%          40%          45%          50%          55% \n-0.986158344 -0.800025977 -0.596640550 -0.401215686 -0.217399848 -0.002404445 \n         60%          65%          70%          75%          80%          85% \n 0.234612085  0.487738521  0.736773063  1.008367577  1.327689814  1.733745438 \n         90%          95%          96%          97%          98%        98.5% \n 2.225818652  2.908677146  3.102227281  3.292356325  3.593860825  3.885859410 \n         99%        99.5%         100% \n 4.114345227  4.562982520  6.832739065 \n\n\n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_SORate_old\n\n        0%         5%        10%        15%        20%        25%        30% \n0.02599653 0.06809067 0.08259171 0.09310484 0.10099334 0.11001969 0.11730861 \n       35%        40%        45%        50%        55%        60%        65% \n0.12377639 0.12957403 0.13578392 0.14170040 0.14766230 0.15363483 0.16165850 \n       70%        75%        80%        85%        90%        95%        96% \n0.16866649 0.17689917 0.18550138 0.19829709 0.21331720 0.23486239 0.24086542 \n       97%        98%      98.5%        99%      99.5%       100% \n0.25079023 0.26288020 0.26998529 0.28194742 0.29799283 0.38765432 \n\n\n\npercentiles_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_SORate_new\n\n        0%         5%        10%        15%        20%        25%        30% \n0.04245974 0.08805424 0.10183586 0.11183144 0.12169312 0.12979890 0.13868421 \n       35%        40%        45%        50%        55%        60%        65% \n0.14614294 0.15322835 0.15976331 0.16608997 0.17353108 0.18151769 0.18917430 \n       70%        75%        80%        85%        90%        95%        96% \n0.19730342 0.20579268 0.21667851 0.22910250 0.24463240 0.27045781 0.27703585 \n       97%        98%      98.5%        99%      99.5%       100% \n0.28755084 0.30055950 0.30908875 0.32068440 0.33275529 0.36565097 \n\n\n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_OPS_old\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.45000 0.60490 0.63900 0.66000 0.67900 0.69400 0.70700 0.72100 0.73300 0.74700 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.76000 0.77200 0.78400 0.79800 0.81400 0.83200 0.85000 0.87500 0.90420 0.95710 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.97336 0.99400 1.02000 1.03213 1.05942 1.09042 1.22200 \n\n\n\npercentiles_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_OPS_new\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.48800 0.62340 0.65400 0.67400 0.68900 0.70300 0.71600 0.72800 0.74100 0.75300 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.76500 0.77800 0.78800 0.80200 0.81500 0.83100 0.85100 0.87300 0.89900 0.94960 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.96168 0.97976 1.00484 1.01688 1.04092 1.08384 1.42100 \n\n\n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_age_old\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n   19    23    24    25    26    26    27    27    28    28    29    29    30 \n  65%   70%   75%   80%   85%   90%   95%   96%   97%   98% 98.5%   99% 99.5% \n   30    31    31    32    33    34    36    37    37    38    39    39    40 \n 100% \n   44 \n\n\n\npercentiles_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_age_new\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n   19    23    24    25    26    26    27    27    28    28    29    29    30 \n  65%   70%   75%   80%   85%   90%   95%   96%   97%   98% 98.5%   99% 99.5% \n   31    31    32    33    34    35    36    37    37    38    39    40    40 \n 100% \n   47 \n\n\n3 metrics that were especially looked at when comparing these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being based on statistics but then pondering the top 10%, 5%, 1% of players, these values were also noted to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that were chosen and there is no significant distinction between them just based on the eye test i.e. better statistics can be ruled out as a possible cause of any pay disparity.\nLastly, the correlation matrix was created to see if there are any statistics that are worth looking at for similarity to combine into one variable similar to what was done to PC_Score. This led to finding a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that PC_score is already a heavy hitter stat, OPS and BA were not combined into a single variable.\nFor example, Ichiro, a dominant contact hitter with a league leading 0.350 BA, would be over looked if BA were omitted because as a power hitter, he was not nearly as powerful as the other players and he would get overlooked in the database if the league leading contact part of his game were omitted.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")"
  },
  {
    "objectID": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "href": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "title": "Baseball",
    "section": "Here I will modify the dataset a little bit to get the variables that I want.",
    "text": "Here I will modify the dataset a little bit to get the variables that I want.\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nSince a HR when successfully achieved in baseball also counts as an RBI and R, I decided to combine them into a single variable.\n\nCreating a single variable for HR, RBI, and R\nBelow, we will create the PC variable called PC_Score which will use principal component analysis to create an equation for all 3 variables and combine them into a single variable.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_bkMMo\n2 RBI    0.607 PC1       pca_bkMMo\n3 R      0.539 PC1       pca_bkMMo\n4 HR     0.518 PC2       pca_bkMMo\n5 RBI    0.232 PC2       pca_bkMMo\n6 R     -0.823 PC2       pca_bkMMo\n7 HR     0.625 PC3       pca_bkMMo\n8 RBI   -0.760 PC3       pca_bkMMo\n9 R      0.178 PC3       pca_bkMMo\n\n\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))"
  },
  {
    "objectID": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "href": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "title": "Baseball",
    "section": "Here, we now add the PC_Score variable to our dataset",
    "text": "Here, we now add the PC_Score variable to our dataset\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow, I will take the salary variable and turn it into a z-score to account for the change in salary due to inflation. i.e. I don’t want inflation to undermine what I am trying to accomplish comparing eras because an increase in nominal salary could easily just be a function of inflation of the dollar and nothing related to the skill of the player.\n\nbat_old &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_bkMMo\n2 RBI    0.607 PC1       pca_bkMMo\n3 R      0.539 PC1       pca_bkMMo\n4 HR     0.518 PC2       pca_bkMMo\n5 RBI    0.232 PC2       pca_bkMMo\n6 R     -0.823 PC2       pca_bkMMo\n7 HR     0.625 PC3       pca_bkMMo\n8 RBI   -0.760 PC3       pca_bkMMo\n9 R      0.178 PC3       pca_bkMMo\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\nThen repeating the process of creating a data set and PC_Score for the new era of players.\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nLooking at the top of the list and seeing the heaviest hitters Barry Bonds, Sammy Sosa, and Alex Rodriguez on the list for PC_Score gave me the idea that the list made sense. I also looked for a player like Ichiro, one of the most talented contact hitters, to see how he would be evaluated. Given that he was placed in the upper-middle of the pack, it made sense that he would not be featured with the heaviest sluggers in the league.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nBelow we are adding standard deviation of salary.\n\nbat_new &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_Wo635\n2 RBI    0.604 PC1       pca_Wo635\n3 R      0.544 PC1       pca_Wo635\n4 HR    -0.531 PC2       pca_Wo635\n5 RBI   -0.225 PC2       pca_Wo635\n6 R      0.817 PC2       pca_Wo635\n7 HR    -0.616 PC3       pca_Wo635\n8 RBI    0.765 PC3       pca_Wo635\n9 R     -0.189 PC3       pca_Wo635\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\n\nWe are now going to adjust the Salary_ZScore values in our data frames to not include N/A values as values that we are interested in observing so that graphically, we won’t have any issues below.\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nNow, one of the questions that I wanted to explore using the eye test first was if there was a difference in any stat categories between the era’s that might account for the cause of a salary difference. I created the percentiles of some of the major stat categories below and compared them between the older and newer datasets.\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n3 metrics that I especially looked at when I wanted to compared these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being base on statistics but then I wonder about just the top 10%, 5%, 1% of players to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that I chose and there is no significant distinction between them just based on the eye test, so we can rule out different offensive numbers as a possible cause of any pay disparity.\nHere are some graphs of the percentiles above to visually show also that they do not appear to be very different from each other.\n\necdf(bat_old$BA)\n\nEmpirical CDF \nCall: ecdf(bat_old$BA)\n x[1:179] =  0.172,  0.179,  0.181,  ...,  0.379,  0.394\n\nplot(ecdf(bat_old$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$age)\n\nEmpirical CDF \nCall: ecdf(bat_old$age)\n x[1:26] =     19,     20,     21,  ...,     43,     44\n\nplot(ecdf(bat_old$age))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PA)\n\nEmpirical CDF \nCall: ecdf(bat_old$PA)\n x[1:511] =    214,    219,    221,  ...,    758,    773\n\nplot(ecdf(bat_old$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_old$PC_score)\n x[1:3266] = -2.9923, -2.8918, -2.8604,  ..., 6.3425, 6.4562\n\nplot(ecdf(bat_old$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_old$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_old$OPS)\n x[1:683] =   0.45,  0.465,  0.466,  ...,  1.216,  1.222\n\nplot(ecdf(bat_old$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_old$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_old$SORate)\n x[1:2991] = 0.025997, 0.027237, 0.02812,  ..., 0.36406, 0.38765\n\nplot(ecdf(bat_old$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_old$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_old$Salary_ZScore)\n x[1:2315] = -1.2453, -1.245, -1.2447,  ..., 3.6882, 3.7975\n\nplot(ecdf(bat_old$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_new$Salary_ZScore)\n x[1:2410] = -0.95118, -0.95065, -0.94986,  ..., 5.2009, 5.2309\n\nplot(ecdf(bat_new$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$BA)\n\nEmpirical CDF \nCall: ecdf(bat_new$BA)\n x[1:176] =  0.159,  0.174,  0.178,  ...,   0.37,  0.372\n\nplot(ecdf(bat_new$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PA)\n\nEmpirical CDF \nCall: ecdf(bat_new$PA)\n x[1:518] =    215,    221,    222,  ...,    765,    778\n\nplot(ecdf(bat_new$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_new$PC_score)\n x[1:3565] = -3.1599, -3.1129, -3.1032,  ..., 6.4042, 6.8327\n\nplot(ecdf(bat_new$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_new$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_new$SORate)\n x[1:3272] = 0.04246, 0.045455, 0.046791,  ..., 0.36239, 0.36565\n\nplot(ecdf(bat_new$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_new$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_new$OPS)\n x[1:661] =  0.488,  0.521,   0.53,  ...,  1.381,  1.421\n\nplot(ecdf(bat_new$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_new$age)\n\nEmpirical CDF \nCall: ecdf(bat_new$age)\n x[1:27] =     20,     21,     22,  ...,     46,     47\n\nplot(ecdf(bat_new$age))\n\n\n\n\n\n\n\n\nEXPERIMENT\nBriefly plotting the correlation matrix to see if there are any statistics that are worth looking at for similarity to combine into one variable led me to a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that I already have a heavy hitter stat for PC_score, I decided to keep these variables separate to account for this difference. For example, Ichiro was as dominant as he was due to his league leading 0.350+ BA but if you looked at OPS, we was about 0.82, which barely cracks the top 25% of players and he is most definitely a player that was in the top 5% of players in the league and a former league MVP. For this reason, I kept the two metrics separate.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nThe scatter plots below I do not believe are particularly relevant to the salary discussion because of how much noise there are in the graphs but to show that they do not really contribute to the salary discussion, I plotted them below to at least see the visual data.\n\nplot(bat_new$BA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$age, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$BA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$SORate, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$age)"
  },
  {
    "objectID": "Baseball.html#modeling",
    "href": "Baseball.html#modeling",
    "title": "Baseball",
    "section": "Modeling",
    "text": "Modeling\nNow to actually look at the salary variable in a ridge regression model, first the old and new data sets will be split into 4 data sets called test_old, test_new, train_old, and train_new.\n\nn&lt;-nrow(bat_old)\nm&lt;-nrow(bat_new)\n\nindices_old&lt;-sample(n, 0.8*floor(n), replace=F)\nindices_new&lt;-sample(m, 0.8*floor(m), replace=F)\n\ntest_old&lt;- bat_old[-indices_old, ]\ntrain_old&lt;- bat_old[indices_old, ]\n\ntest_new&lt;- bat_new[-indices_new, ]\ntrain_new &lt;- bat_new[indices_new, ]\n\nFirst, the ridge model for the old data set is created below.\n\nridge_model_old &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_old &lt;- workflow() |&gt;\n  add_model(ridge_model_old)\n\nThe desired variables are then added into the recipe.\n\nridge_recipe_old &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_old\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_old &lt;- ridge_wflow_old |&gt;\n  add_recipe(ridge_recipe_old)\n\nThen, the recipe is cross validated and tuned.\n\nset.seed(1332)\nbatting_cv_old &lt;- vfold_cv(train_old, v = 10)\n\nridge_tune_old &lt;- tune_grid(ridge_model_old, \n                         ridge_recipe_old,\n                      resamples = batting_cv_old, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\nThe desired tuning parameters are pulled from the model.\n\nridge_tune_old |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nThe model is then finalized.\n\nridge_best_old &lt;- ridge_tune_old |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_old\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.244 Preprocessor1_Model18\n\nridge_wflow_final_old &lt;- ridge_wflow_old |&gt;\n  finalize_workflow(parameters = ridge_best_old) \n\nridge_wflow_final_old &lt;- fit(ridge_wflow_final_old, data=train_old) \nridge_wflow_final_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 479.40\n2    6  0.35 436.80\n3    6  0.39 398.00\n4    6  0.43 362.60\n5    6  0.47 330.40\n6    6  0.51 301.10\n7    6  0.56 274.30\n8    6  0.61 249.90\n9    6  0.67 227.70\n10   6  0.74 207.50\n11   6  0.81 189.10\n12   6  0.89 172.30\n13   6  0.97 157.00\n14   6  1.06 143.00\n15   6  1.17 130.30\n16   6  1.28 118.70\n17   6  1.40 108.20\n18   6  1.53  98.58\n19   6  1.67  89.83\n20   6  1.83  81.85\n21   6  2.00  74.57\n22   6  2.18  67.95\n23   6  2.39  61.91\n24   6  2.61  56.41\n25   6  2.85  51.40\n26   6  3.10  46.84\n27   6  3.39  42.67\n28   6  3.69  38.88\n29   6  4.02  35.43\n30   6  4.37  32.28\n31   6  4.76  29.41\n32   6  5.17  26.80\n33   6  5.61  24.42\n34   6  6.09  22.25\n35   6  6.60  20.27\n36   6  7.15  18.47\n37   6  7.73  16.83\n38   6  8.35  15.34\n39   6  9.01  13.97\n40   6  9.71  12.73\n41   6 10.45  11.60\n42   6 11.22  10.57\n43   6 12.04   9.63\n44   6 12.90   8.78\n45   6 13.79   8.00\n46   6 14.72   7.29\n\n...\nand 54 more lines.\n\n\nThe predictions are made and a calibration plot is made to make sure that the model works.\n\nridge_pred_check_old &lt;- ridge_wflow_final_old |&gt;\n  fit_resamples(\n    resamples = batting_cv_old,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_old, y_pred = predict(ridge_wflow_final_old, new_data = test_old))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow, the finals pieces of the model are computed.\n\nridge_fit_old &lt;- ridge_wflow_final_old |&gt;\n  fit(data = train_old)\nridge_fit_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 479.40\n2    6  0.35 436.80\n3    6  0.39 398.00\n4    6  0.43 362.60\n5    6  0.47 330.40\n6    6  0.51 301.10\n7    6  0.56 274.30\n8    6  0.61 249.90\n9    6  0.67 227.70\n10   6  0.74 207.50\n11   6  0.81 189.10\n12   6  0.89 172.30\n13   6  0.97 157.00\n14   6  1.06 143.00\n15   6  1.17 130.30\n16   6  1.28 118.70\n17   6  1.40 108.20\n18   6  1.53  98.58\n19   6  1.67  89.83\n20   6  1.83  81.85\n21   6  2.00  74.57\n22   6  2.18  67.95\n23   6  2.39  61.91\n24   6  2.61  56.41\n25   6  2.85  51.40\n26   6  3.10  46.84\n27   6  3.39  42.67\n28   6  3.69  38.88\n29   6  4.02  35.43\n30   6  4.37  32.28\n31   6  4.76  29.41\n32   6  5.17  26.80\n33   6  5.61  24.42\n34   6  6.09  22.25\n35   6  6.60  20.27\n36   6  7.15  18.47\n37   6  7.73  16.83\n38   6  8.35  15.34\n39   6  9.01  13.97\n40   6  9.71  12.73\n41   6 10.45  11.60\n42   6 11.22  10.57\n43   6 12.04   9.63\n44   6 12.90   8.78\n45   6 13.79   8.00\n46   6 14.72   7.29\n\n...\nand 54 more lines.\n\n\n\nridge_fit_old |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_old &lt;- ridge_fit_old |&gt;\n  broom::tidy()\nridge_coef_old\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.00618   0.244\n2 BA          -0.0619    0.244\n3 PA           0.159     0.244\n4 PC_score     0.225     0.244\n5 OPS          0.120     0.244\n6 SORate      -0.0653    0.244\n7 age          0.377     0.244\n\n\nThe ridge regression model has been completed below.\n\npredictions_ridge_old &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_old |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 667 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Al M…      -0.00646 -0.110   1997 bottom         477 0.291    13    29 2.27e6\n 2 Alan…      -0.0853   0.0198  1986 middle         361 0.257     7    35 5.25e5\n 3 Albe…       3.44     0.879   2000 lower          622 0.281    23    34 1.29e7\n 4 Alex…      -0.767   -0.826   1996 bottom         246 0.277     3    28 4.13e5\n 5 Alex…      -0.781   -0.665   1993 lower          399 0.256     0    28 3.5 e5\n 6 Alex…      -0.898   -1.11    1995 middle         425 0.243    10    22 1.3 e5\n 7 Alex…      -0.877   -0.663   1999 bottom         591 0.277    14    22 2.01e5\n 8 Alex…      -0.722   -0.322   2000 middle         275 0.316    13    28 8.63e5\n 9 Andr…       1.98     1.15    1991 upper          596 0.272    31    37 3.32e6\n10 Andr…       1.21     0.703   1986 bottom         475 0.229    17    37 1.10e6\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.752\n\n\n\nridge_model_new &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_new &lt;- workflow() |&gt;\n  add_model(ridge_model_new)\n\nThe same process is repeated for the ‘new’ model. First with the recipe.\n\nridge_recipe_new &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_new\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_new &lt;- ridge_wflow_new |&gt;\n  add_recipe(ridge_recipe_new)\n\nThen, the new model is tuned and cross validated.\n\nset.seed(1332)\nbatting_cv_new &lt;- vfold_cv(train_new, v = 10)\n\nridge_tune_new &lt;- tune_grid(ridge_model_new, \n                         ridge_recipe_new,\n                      resamples = batting_cv_new, \n                      grid = grid_regular(penalty(range = c(-1, 2.5)), levels = 50))\n\nThe metrics are collected.\n\nridge_tune_new |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nThe workflow is now updated with the new ‘rmse’ value.\n\nridge_best_new &lt;- ridge_tune_new |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_new\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.439 Preprocessor1_Model10\n\nridge_wflow_final_new &lt;- ridge_wflow_new |&gt;\n  finalize_workflow(parameters = ridge_best_new) \n\nridge_wflow_final_new &lt;- fit(ridge_wflow_final_new, data=train_new) \n\nThe predictions are made.\n\nridge_pred_check_new &lt;- ridge_wflow_final_new |&gt;\n  fit_resamples(\n    resamples = batting_cv_new,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_new, y_pred = predict(ridge_wflow_final_new, new_data = test_new))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe training data is fit.\n\nridge_fit_new &lt;- ridge_wflow_final_new |&gt;\n  fit(data = train_new)\nridge_fit_new\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_mean()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 461.90\n2    6  0.31 420.90\n3    6  0.34 383.50\n4    6  0.37 349.40\n5    6  0.41 318.40\n6    6  0.45 290.10\n7    6  0.49 264.30\n8    6  0.54 240.80\n9    6  0.59 219.40\n10   6  0.64 199.90\n11   6  0.70 182.20\n12   6  0.77 166.00\n13   6  0.85 151.30\n14   6  0.93 137.80\n15   6  1.02 125.60\n16   6  1.11 114.40\n17   6  1.22 104.30\n18   6  1.33  94.99\n19   6  1.46  86.55\n20   6  1.59  78.86\n21   6  1.74  71.86\n22   6  1.90  65.47\n23   6  2.08  59.66\n24   6  2.27  54.36\n25   6  2.48  49.53\n26   6  2.71  45.13\n27   6  2.95  41.12\n28   6  3.22  37.47\n29   6  3.51  34.14\n30   6  3.82  31.10\n31   6  4.15  28.34\n32   6  4.52  25.82\n33   6  4.91  23.53\n34   6  5.32  21.44\n35   6  5.77  19.53\n36   6  6.25  17.80\n37   6  6.77  16.22\n38   6  7.31  14.78\n39   6  7.89  13.46\n40   6  8.51  12.27\n41   6  9.16  11.18\n42   6  9.85  10.19\n43   6 10.57   9.28\n44   6 11.33   8.46\n45   6 12.13   7.70\n46   6 12.96   7.02\n\n...\nand 54 more lines.\n\n\nThe lambda values are plotted.\n\nridge_fit_new |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nThe updated co-efficients are added.\n\nridge_coef_new &lt;- ridge_fit_new |&gt;\n  broom::tidy()\nridge_coef_new\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.0180    0.439\n2 BA          -0.00865   0.439\n3 PA           0.125     0.439\n4 PC_score     0.171     0.439\n5 OPS          0.104     0.439\n6 SORate      -0.0161    0.439\n7 age          0.329     0.439\n\n\nThe predictions are made.\n\npredictions_ridge_new &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…       -0.922  -0.717   2001 lower          405 0.289     7    24 2.10e5\n 2 A. J…       -0.342  -0.219   2005 upper          497 0.257    18    28 2.25e6\n 3 A. J…        0.354   0.0755  2010 upper          503 0.27      9    33 6.75e6\n 4 Aaro…       -0.868  -0.229   2001 middle         427 0.294    14    28 4   e5\n 5 Aaro…       -0.0117 -0.0131  2003 middle         446 0.273    18    30 3.7 e6\n 6 Aaro…       -0.789  -0.500   2006 middle         606 0.291     6    24 3.36e5\n 7 Aaro…       -0.147  -0.0833  2010 middle         580 0.205    26    28 4   e6\n 8 Aaro…        0.837   0.0163  2014 lower          541 0.244    10    32 1.10e7\n 9 Aaro…        1.60   -0.177   2010 upper          357 0.23     11    33 1.36e7\n10 Aaro…        1.63   -0.245   2011 upper          351 0.233     4    34 1.36e7\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;"
  },
  {
    "objectID": "Baseball.html#insights",
    "href": "Baseball.html#insights",
    "title": "Baseball",
    "section": "Insights",
    "text": "Insights\nI saw that the model did hint at a possibility that there was a marginal difference even though we cannot reject the null hypothesis. That being said, there are a few takeaways from this experiment that I gathered. One is that the distributions of the batting statistics across the board for both era’s are very similar at each percentile range. The players are very consistently producing a very similar distribution of HR, BA, etc year after year. Also, the salaries are very skewed for the league best players and the best players as expected would receive the highest pay.\n\nLimitations and Future Work\nThe clear limitation to this project would be the overall limited-ness of the salary database. As mentioned previously, there was not a very large data set for salaries since they only went back to the 1980s. There was also no clear distinction of the salary cap of each team during this time and no mentions of a player getting payed more because he played for the Yankees for example. Future work could include just looking at the top few teams in the game to separate this dataset even more to account for the fact that the most valuable teams will pay their players the most money.\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "Baseball.html#data-limitations",
    "href": "Baseball.html#data-limitations",
    "title": "Baseball",
    "section": "",
    "text": "Due to the size of the Salary database, I was only able to look at the years from 1985-2016."
  },
  {
    "objectID": "Baseball.html#experiment",
    "href": "Baseball.html#experiment",
    "title": "Baseball",
    "section": "Experiment",
    "text": "Experiment\nIn this experiment, the test set from the ‘new’ era will be placed into the ‘old’ model and vice versa to see, which model predicts larger salary z_score values.\npredictions_ridge_old_new will be the ‘old model’ with ‘new data’ predictions_ridge_new will be the ‘new model’ with the ‘new data’\n\npredictions_ridge_old_new &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_old_new |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 728 × 30\n   Name        Salary_ZScore    .pred  Year team_payroll    PA    BA    HR   age\n   &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 A. J. Pier…       -0.922  -0.831    2001 lower          405 0.289     7    24\n 2 A. J. Pier…       -0.342  -0.174    2005 upper          497 0.257    18    28\n 3 A. J. Pier…        0.354   0.181    2010 upper          503 0.27      9    33\n 4 Aaron Boone       -0.868  -0.296    2001 middle         427 0.294    14    28\n 5 Aaron Boone       -0.0117 -0.00165  2003 middle         446 0.273    18    30\n 6 Aaron Hill        -0.789  -0.524    2006 middle         606 0.291     6    24\n 7 Aaron Hill        -0.147   0.0919   2010 middle         580 0.205    26    28\n 8 Aaron Hill         0.837   0.0834   2014 lower          541 0.244    10    32\n 9 Aaron Rowa…        1.60   -0.187    2010 upper          357 0.23     11    33\n10 Aaron Rowa…        1.63   -0.316    2011 upper          351 0.233     4    34\n# ℹ 718 more rows\n# ℹ 21 more variables: salary &lt;int&gt;, R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;,\n#   OBP &lt;dbl&gt;, BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name        Salary_ZScore    .pred  Year team_payroll    PA    BA    HR   age\n   &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 A. J. Pier…       -0.922  -0.831    2001 lower          405 0.289     7    24\n 2 A. J. Pier…       -0.342  -0.174    2005 upper          497 0.257    18    28\n 3 A. J. Pier…        0.354   0.181    2010 upper          503 0.27      9    33\n 4 Aaron Boone       -0.868  -0.296    2001 middle         427 0.294    14    28\n 5 Aaron Boone       -0.0117 -0.00165  2003 middle         446 0.273    18    30\n 6 Aaron Hill        -0.789  -0.524    2006 middle         606 0.291     6    24\n 7 Aaron Hill        -0.147   0.0919   2010 middle         580 0.205    26    28\n 8 Aaron Hill         0.837   0.0834   2014 lower          541 0.244    10    32\n 9 Aaron Rowa…        1.60   -0.187    2010 upper          357 0.23     11    33\n10 Aaron Rowa…        1.63   -0.316    2011 upper          351 0.233     4    34\n# ℹ 718 more rows\n# ℹ 21 more variables: salary &lt;int&gt;, R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;,\n#   OBP &lt;dbl&gt;, BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.831\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.812\n\n\npredictions_ridge_new_old will be the ‘new model’ with the ‘old data’ predictions_ridge_old will be the ‘old model’ with the ‘old data’\n\npredictions_ridge_new_old &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_new_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name        Salary_ZScore    .pred  Year team_payroll    PA    BA    HR   age\n   &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 Al Martin        -0.00646 -0.0728   1997 bottom         477 0.291    13    29\n 2 Alan Ashby       -0.0853   0.00932  1986 middle         361 0.257     7    35\n 3 Albert Bel…       3.44     0.681    2000 lower          622 0.281    23    34\n 4 Alex Arias       -0.767   -0.716    1996 bottom         246 0.277     3    28\n 5 Alex Cole        -0.781   -0.606    1993 lower          399 0.256     0    28\n 6 Alex Gonza…      -0.898   -0.928    1995 middle         425 0.243    10    22\n 7 Alex Gonza…      -0.877   -0.589    1999 bottom         591 0.277    14    22\n 8 Alex Ochoa       -0.722   -0.250    2000 middle         275 0.316    13    28\n 9 Andre Daws…       1.98     0.918    1991 upper          596 0.272    31    37\n10 Andre Thor…       1.21     0.514    1986 bottom         475 0.229    17    37\n# ℹ 657 more rows\n# ℹ 21 more variables: salary &lt;int&gt;, R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;,\n#   OBP &lt;dbl&gt;, BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Al M…      -0.00646 -0.110   1997 bottom         477 0.291    13    29 2.27e6\n 2 Alan…      -0.0853   0.0198  1986 middle         361 0.257     7    35 5.25e5\n 3 Albe…       3.44     0.879   2000 lower          622 0.281    23    34 1.29e7\n 4 Alex…      -0.767   -0.826   1996 bottom         246 0.277     3    28 4.13e5\n 5 Alex…      -0.781   -0.665   1993 lower          399 0.256     0    28 3.5 e5\n 6 Alex…      -0.898   -1.11    1995 middle         425 0.243    10    22 1.3 e5\n 7 Alex…      -0.877   -0.663   1999 bottom         591 0.277    14    22 2.01e5\n 8 Alex…      -0.722   -0.322   2000 middle         275 0.316    13    28 8.63e5\n 9 Andr…       1.98     1.15    1991 upper          596 0.272    31    37 3.32e6\n10 Andr…       1.21     0.703   1986 bottom         475 0.229    17    37 1.10e6\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.780\n\n\nAs it turns out, there is a difference between the models and the older model predicts higher salary z-scores for both models indicating that the older players are predicted to be payed more than the new players thus showing that there is a pay disparity between the older group of players during the steroid era and the newer group of players.\nNow, I will check for the significance of this disparity to see if any conclusions can be drawn from this model by conducting a two-sample t-test.\nFirst it will judge based on the ‘new data’\n\ngroupA&lt;-predictions_ridge_old_new$.pred\ngroupB&lt;-predictions_ridge_new$.pred\n\n\nt.test(groupA, groupB, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  groupA and groupB\nt = 1.1252, df = 1416.8, p-value = 0.2607\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.02308555  0.08519096\nsample estimates:\n mean of x  mean of y \n0.05420462 0.02315192 \n\n\nThen, it will judge based on the ‘old data’\n\ngroup1&lt;-predictions_ridge_old$.pred\ngroup2&lt;-predictions_ridge_new_old$.pred\n\nt.test(group1, group2, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 1.3502, df = 1304.1, p-value = 0.1772\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.01734762  0.09395267\nsample estimates:\n  mean of x   mean of y \n-0.02235101 -0.06065354 \n\n\nUpon further investigation, the two sample t.test shows that there is not enough evidence to support the alternative hypothesis that there is a difference of means and thus we conclude not only that the players in the old data set and the new data set have no significant difference in stats but adjusting for inflation no significant differences in salaries as well. With p-values of .35 and .23, we can conclude that there is most likely a weak correlation between old players and higher salaries but not anything conclusive enough to overturn the null hypothesis."
  }
]