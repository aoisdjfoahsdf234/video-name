[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Baseball Analytics",
    "section": "",
    "text": "Brian’s website https://websitename_name_html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Baseball.html",
    "href": "Baseball.html",
    "title": "Baseball",
    "section": "",
    "text": "Include that my baseball salaries only went back to 1980 and up to 2016, left out many key years."
  },
  {
    "objectID": "Baseball.html#exploratory-data-analysis",
    "href": "Baseball.html#exploratory-data-analysis",
    "title": "Baseball",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDefining my era’s\nThe first thing that I need to do is define my eras of baseball. Since the salary database only goes back to 1985, I divided my old and new eras into 15 year windows from 1985-2000 and 2001-2016\n\nlibrary(Lahman)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nsalaries &lt;- Salaries |&gt;\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People |&gt;\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() |&gt;\n  left_join(salaries, \n            by =c(\"playerID\", \"yearID\", \"teamID\")) |&gt;\n  left_join(peopleInfo, by = \"playerID\") |&gt;\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) |&gt;\n  arrange(playerID, yearID, stint)\n\nbatting_older &lt;- batting |&gt; filter(G &gt;= 100, AB &gt;= 200, 2001 &gt; yearID, yearID &gt; 1984) |&gt;\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_older &lt;- batting_older |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))"
  },
  {
    "objectID": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "href": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "title": "Baseball",
    "section": "Here I will modify the dataset a little bit to get the variables that I want.",
    "text": "Here I will modify the dataset a little bit to get the variables that I want.\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nSince a HR when successfully achieved in baseball also counts as an RBI and R, I decided to combine them into a single variable.\n\nCreating a single variable for HR, RBI, and R\nBelow, we will create the PC variable called PC_Score which will use principal component analysis to create an equation for all 3 variables and combine them into a single variable.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_nBtzL\n2 RBI    0.607 PC1       pca_nBtzL\n3 R      0.539 PC1       pca_nBtzL\n4 HR     0.518 PC2       pca_nBtzL\n5 RBI    0.232 PC2       pca_nBtzL\n6 R     -0.823 PC2       pca_nBtzL\n7 HR     0.625 PC3       pca_nBtzL\n8 RBI   -0.760 PC3       pca_nBtzL\n9 R      0.178 PC3       pca_nBtzL\n\n\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))"
  },
  {
    "objectID": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "href": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "title": "Baseball",
    "section": "Here, we now add the PC_Score variable to our dataset",
    "text": "Here, we now add the PC_Score variable to our dataset\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow, I will take the salary variable and turn it into a z-score to account for the change in salary due to inflation. i.e. I don’t want inflation to undermine what I am trying to accomplish comparing eras because an increase in nominal salary could easily just be a function of inflation of the dollar and nothing related to the skill of the player.\n\nbat_old &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_nBtzL\n2 RBI    0.607 PC1       pca_nBtzL\n3 R      0.539 PC1       pca_nBtzL\n4 HR     0.518 PC2       pca_nBtzL\n5 RBI    0.232 PC2       pca_nBtzL\n6 R     -0.823 PC2       pca_nBtzL\n7 HR     0.625 PC3       pca_nBtzL\n8 RBI   -0.760 PC3       pca_nBtzL\n9 R      0.178 PC3       pca_nBtzL\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\nThen repeating the process of creating a data set and PC_Score for the new era of players.\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nLooking at the top of the list and seeing the heaviest hitters Barry Bonds, Sammy Sosa, and Alex Rodriguez on the list for PC_Score gave me the idea that the list made sense. I also looked for a player like Ichiro, one of the most talented contact hitters, to see how he would be evaluated. Given that he was placed in the upper-middle of the pack, it made sense that he would not be featured with the heaviest sluggers in the league.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nBelow we are adding standard deviation of salary.\n\nbat_new &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_XjiMp\n2 RBI    0.604 PC1       pca_XjiMp\n3 R      0.544 PC1       pca_XjiMp\n4 HR    -0.531 PC2       pca_XjiMp\n5 RBI   -0.225 PC2       pca_XjiMp\n6 R      0.817 PC2       pca_XjiMp\n7 HR    -0.616 PC3       pca_XjiMp\n8 RBI    0.765 PC3       pca_XjiMp\n9 R     -0.189 PC3       pca_XjiMp\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\n\nWe are now going to adjust the Salary_ZScore values in our data frames to not include N/A values as values that we are interested in observing so that graphically, we won’t have any issues below.\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nNow, one of the questions that I wanted to explore using the eye test first was if there was a difference in any stat categories between the era’s that might account for the cause of a salary difference. I created the percentiles of some of the major stat categories below and compared them between the older and newer datasets.\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n3 metrics that I especially looked at when I wanted to compared these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being base on statistics but then I wonder about just the top 10%, 5%, 1% of players to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that I chose and there is no significant distinction between them just based on the eye test, so we can rule out different offensive numbers as a possible cause of any pay disparity.\nHere are some graphs of the percentiles above to visually show also that they do not appear to be very different from each other.\n\necdf(bat_old$BA)\n\nEmpirical CDF \nCall: ecdf(bat_old$BA)\n x[1:179] =  0.172,  0.179,  0.181,  ...,  0.379,  0.394\n\nplot(ecdf(bat_old$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$age)\n\nEmpirical CDF \nCall: ecdf(bat_old$age)\n x[1:26] =     19,     20,     21,  ...,     43,     44\n\nplot(ecdf(bat_old$age))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PA)\n\nEmpirical CDF \nCall: ecdf(bat_old$PA)\n x[1:511] =    214,    219,    221,  ...,    758,    773\n\nplot(ecdf(bat_old$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_old$PC_score)\n x[1:3266] = -2.9923, -2.8918, -2.8604,  ..., 6.3425, 6.4562\n\nplot(ecdf(bat_old$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_old$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_old$OPS)\n x[1:683] =   0.45,  0.465,  0.466,  ...,  1.216,  1.222\n\nplot(ecdf(bat_old$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_old$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_old$SORate)\n x[1:2991] = 0.025997, 0.027237, 0.02812,  ..., 0.36406, 0.38765\n\nplot(ecdf(bat_old$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_old$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_old$Salary_ZScore)\n x[1:2315] = -1.2453, -1.245, -1.2447,  ..., 3.6882, 3.7975\n\nplot(ecdf(bat_old$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_new$Salary_ZScore)\n x[1:2410] = -0.95118, -0.95065, -0.94986,  ..., 5.2009, 5.2309\n\nplot(ecdf(bat_new$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$BA)\n\nEmpirical CDF \nCall: ecdf(bat_new$BA)\n x[1:176] =  0.159,  0.174,  0.178,  ...,   0.37,  0.372\n\nplot(ecdf(bat_new$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PA)\n\nEmpirical CDF \nCall: ecdf(bat_new$PA)\n x[1:518] =    215,    221,    222,  ...,    765,    778\n\nplot(ecdf(bat_new$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_new$PC_score)\n x[1:3565] = -3.1599, -3.1129, -3.1032,  ..., 6.4042, 6.8327\n\nplot(ecdf(bat_new$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_new$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_new$SORate)\n x[1:3272] = 0.04246, 0.045455, 0.046791,  ..., 0.36239, 0.36565\n\nplot(ecdf(bat_new$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_new$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_new$OPS)\n x[1:661] =  0.488,  0.521,   0.53,  ...,  1.381,  1.421\n\nplot(ecdf(bat_new$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_new$age)\n\nEmpirical CDF \nCall: ecdf(bat_new$age)\n x[1:27] =     20,     21,     22,  ...,     46,     47\n\nplot(ecdf(bat_new$age))\n\n\n\n\n\n\n\n\nEXPERIMENT\nBriefly plotting the correlation matrix to see if there are any statistics that are worth looking at for similarity to combine into one variable led me to a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that I already have a heavy hitter stat for PC_score, I decided to keep these variables separate to account for this difference. For example, Ichiro was as dominant as he was due to his league leading 0.350+ BA but if you looked at OPS, we was about 0.82, which barely cracks the top 25% of players and he is most definitely a player that was in the top 5% of players in the league and a former league MVP. For this reason, I kept the two metrics separate.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nThe scatter plots below I do not believe are particularly relevant to the salary discussion because of how much noise there are in the graphs but to show that they do not really contribute to the salary discussion, I plotted them below to at least see the visual data.\n\nplot(bat_new$BA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$age, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$BA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$SORate, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$age)"
  },
  {
    "objectID": "Baseball.html#modeling",
    "href": "Baseball.html#modeling",
    "title": "Baseball",
    "section": "Modeling",
    "text": "Modeling\nNow to actually look at the salary variable in a ridge regression model, first the old and new data sets will be split into 4 data sets called test_old, test_new, train_old, and train_new.\n\nn&lt;-nrow(bat_old)\nm&lt;-nrow(bat_new)\n\nindices_old&lt;-sample(n, 0.8*floor(n), replace=F)\nindices_new&lt;-sample(m, 0.8*floor(m), replace=F)\n\ntest_old&lt;- bat_old[-indices_old, ]\ntrain_old&lt;- bat_old[indices_old, ]\n\ntest_new&lt;- bat_new[-indices_new, ]\ntrain_new &lt;- bat_new[indices_new, ]\n\nLastly, I decided to compute a ridge model, one for the old players and one for the new players to see if there was anything that could be observed for the trends of the salaries based on these two eras.\n\nridge_model_old &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_old &lt;- workflow() |&gt;\n  add_model(ridge_model_old)\n\n\nridge_recipe_old &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_old\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_old &lt;- ridge_wflow_old |&gt;\n  add_recipe(ridge_recipe_old)\n\n\nset.seed(1332)\nbatting_cv_old &lt;- vfold_cv(train_old, v = 10)\n\nridge_tune_old &lt;- tune_grid(ridge_model_old, \n                         ridge_recipe_old,\n                      resamples = batting_cv_old, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\n\nridge_tune_old |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nridge_best_old &lt;- ridge_tune_old |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_old\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.244 Preprocessor1_Model18\n\nridge_wflow_final_old &lt;- ridge_wflow_old |&gt;\n  finalize_workflow(parameters = ridge_best_old) \n\nridge_wflow_final_old &lt;- fit(ridge_wflow_final_old, data=train_old) \nridge_wflow_final_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 484.80\n2    6  0.37 441.80\n3    6  0.40 402.50\n4    6  0.44 366.80\n5    6  0.49 334.20\n6    6  0.53 304.50\n7    6  0.58 277.40\n8    6  0.64 252.80\n9    6  0.70 230.30\n10   6  0.77 209.90\n11   6  0.84 191.20\n12   6  0.92 174.20\n13   6  1.01 158.80\n14   6  1.11 144.70\n15   6  1.21 131.80\n16   6  1.33 120.10\n17   6  1.45 109.40\n18   6  1.59  99.71\n19   6  1.74  90.85\n20   6  1.90  82.78\n21   6  2.08  75.43\n22   6  2.27  68.73\n23   6  2.48  62.62\n24   6  2.71  57.06\n25   6  2.96  51.99\n26   6  3.22  47.37\n27   6  3.52  43.16\n28   6  3.83  39.33\n29   6  4.17  35.83\n30   6  4.54  32.65\n31   6  4.94  29.75\n32   6  5.36  27.11\n33   6  5.82  24.70\n34   6  6.31  22.50\n35   6  6.84  20.51\n36   6  7.41  18.68\n37   6  8.01  17.02\n38   6  8.65  15.51\n39   6  9.33  14.13\n40   6 10.04  12.88\n41   6 10.80  11.73\n42   6 11.60  10.69\n43   6 12.44   9.74\n44   6 13.32   8.88\n45   6 14.23   8.09\n46   6 15.18   7.37\n\n...\nand 54 more lines.\n\n\n\nridge_pred_check_old &lt;- ridge_wflow_final_old |&gt;\n  fit_resamples(\n    resamples = batting_cv_old,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_old, y_pred = predict(ridge_wflow_final_old, new_data = test_old))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nridge_fit_old &lt;- ridge_wflow_final_old |&gt;\n  fit(data = train_old)\nridge_fit_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 484.80\n2    6  0.37 441.80\n3    6  0.40 402.50\n4    6  0.44 366.80\n5    6  0.49 334.20\n6    6  0.53 304.50\n7    6  0.58 277.40\n8    6  0.64 252.80\n9    6  0.70 230.30\n10   6  0.77 209.90\n11   6  0.84 191.20\n12   6  0.92 174.20\n13   6  1.01 158.80\n14   6  1.11 144.70\n15   6  1.21 131.80\n16   6  1.33 120.10\n17   6  1.45 109.40\n18   6  1.59  99.71\n19   6  1.74  90.85\n20   6  1.90  82.78\n21   6  2.08  75.43\n22   6  2.27  68.73\n23   6  2.48  62.62\n24   6  2.71  57.06\n25   6  2.96  51.99\n26   6  3.22  47.37\n27   6  3.52  43.16\n28   6  3.83  39.33\n29   6  4.17  35.83\n30   6  4.54  32.65\n31   6  4.94  29.75\n32   6  5.36  27.11\n33   6  5.82  24.70\n34   6  6.31  22.50\n35   6  6.84  20.51\n36   6  7.41  18.68\n37   6  8.01  17.02\n38   6  8.65  15.51\n39   6  9.33  14.13\n40   6 10.04  12.88\n41   6 10.80  11.73\n42   6 11.60  10.69\n43   6 12.44   9.74\n44   6 13.32   8.88\n45   6 14.23   8.09\n46   6 15.18   7.37\n\n...\nand 54 more lines.\n\n\n\nridge_fit_old |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_old &lt;- ridge_fit_old |&gt;\n  broom::tidy()\nridge_coef_old\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   0.0124   0.244\n2 BA           -0.0691   0.244\n3 PA            0.162    0.244\n4 PC_score      0.243    0.244\n5 OPS           0.118    0.244\n6 SORate       -0.0823   0.244\n7 age           0.375    0.244\n\n\nThe ridge regression model has been completed below.\n\npredictions_ridge_old &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_old |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 667 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adam …       -0.951  -0.283  2000 top            641 0.266     9    24 2.02e5\n 2 Adria…       -0.870  -0.686  1999 top            614 0.275    15    20 2.20e5\n 3 Adria…       -0.922  -0.721  2000 bottom         340 0.315     4    26 2.85e5\n 4 Al Co…       -0.413   0.383  1985 upper          487 0.265    14    33 4   e5\n 5 Al Ma…        0.0937  0.449  1999 bottom         593 0.277    24    31 2.70e6\n 6 Al Ne…       -0.877  -0.219  1989 lower          521 0.253     0    29 1.80e5\n 7 Alan …        0.895   0.201  1991 upper          421 0.248     9    33 2.20e6\n 8 Alber…        3.39    1.50   1998 upper          706 0.328    49    32 1   e7\n 9 Alex …       -0.860  -0.569  1999 &lt;NA&gt;           329 0.3       8    27 2.45e5\n10 Alex …       -0.752   0.177  1996 upper          677 0.358    36    21 4.42e5\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.703\n\n\n\npredictions_ridge_old_new &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_old_new |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 728 × 30\n   Name        Salary_ZScore    .pred  Year team_payroll    PA    BA    HR   age\n   &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 A. J. Ellis       -0.765  -0.0613   2012 top            505 0.27     13    31\n 2 A. J. Pier…       -0.922  -0.826    2001 lower          405 0.289     7    24\n 3 A. J. Pier…       -0.938  -0.663    2002 lower          469 0.3       6    25\n 4 A. J. Pier…       -0.342  -0.152    2005 upper          497 0.257    18    28\n 5 A. J. Pier…       -0.0227 -0.00559  2006 upper          543 0.295    16    29\n 6 A. J. Pier…        0.217  -0.0466   2007 upper          509 0.263    14    30\n 7 Aaron Boone       -0.868  -0.298    2001 middle         427 0.294    14    28\n 8 Aaron Hill         0.0345 -0.435    2011 middle         429 0.225     6    29\n 9 Aaron Hill         0.146   0.628    2012 lower          668 0.302    26    30\n10 Aaron Rowa…       -0.945  -1.00     2002 upper          331 0.258     7    25\n# ℹ 718 more rows\n# ℹ 21 more variables: salary &lt;int&gt;, R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;,\n#   OBP &lt;dbl&gt;, BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.799\n\n\n\nridge_model_new &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_new &lt;- workflow() |&gt;\n  add_model(ridge_model_new)\n\n\nridge_recipe_new &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_new\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_new &lt;- ridge_wflow_new |&gt;\n  add_recipe(ridge_recipe_new)\n\n\nset.seed(1332)\nbatting_cv_new &lt;- vfold_cv(train_new, v = 10)\n\nridge_tune_new &lt;- tune_grid(ridge_model_new, \n                         ridge_recipe_new,\n                      resamples = batting_cv_new, \n                      grid = grid_regular(penalty(range = c(-1, 2.5)), levels = 50))\n\n\nridge_tune_new |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nridge_best_new &lt;- ridge_tune_new |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_new\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.316 Preprocessor1_Model08\n\nridge_wflow_final_new &lt;- ridge_wflow_new |&gt;\n  finalize_workflow(parameters = ridge_best_new) \n\nridge_wflow_final_new &lt;- fit(ridge_wflow_final_new, data=train_new) \n\n\nridge_pred_check_new &lt;- ridge_wflow_final_new |&gt;\n  fit_resamples(\n    resamples = batting_cv_new,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_new, y_pred = predict(ridge_wflow_final_new, new_data = test_new))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nridge_fit_new &lt;- ridge_wflow_final_new |&gt;\n  fit(data = train_new)\nridge_fit_new\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_mean()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 470.10\n2    6  0.31 428.40\n3    6  0.34 390.30\n4    6  0.37 355.60\n5    6  0.41 324.00\n6    6  0.45 295.30\n7    6  0.49 269.00\n8    6  0.54 245.10\n9    6  0.59 223.40\n10   6  0.64 203.50\n11   6  0.71 185.40\n12   6  0.77 169.00\n13   6  0.85 153.90\n14   6  0.93 140.30\n15   6  1.02 127.80\n16   6  1.11 116.50\n17   6  1.22 106.10\n18   6  1.33  96.68\n19   6  1.46  88.09\n20   6  1.60  80.27\n21   6  1.75  73.14\n22   6  1.91  66.64\n23   6  2.09  60.72\n24   6  2.28  55.33\n25   6  2.49  50.41\n26   6  2.71  45.93\n27   6  2.96  41.85\n28   6  3.23  38.13\n29   6  3.52  34.75\n30   6  3.83  31.66\n31   6  4.17  28.85\n32   6  4.53  26.28\n33   6  4.92  23.95\n34   6  5.34  21.82\n35   6  5.79  19.88\n36   6  6.27  18.12\n37   6  6.79  16.51\n38   6  7.34  15.04\n39   6  7.92  13.70\n40   6  8.54  12.49\n41   6  9.20  11.38\n42   6  9.89  10.37\n43   6 10.62   9.45\n44   6 11.38   8.61\n45   6 12.19   7.84\n46   6 13.02   7.15\n\n...\nand 54 more lines.\n\n\n\nridge_fit_new |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_new &lt;- ridge_fit_new |&gt;\n  broom::tidy()\nridge_coef_new\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.00314   0.316\n2 BA          -0.0212    0.316\n3 PA           0.130     0.316\n4 PC_score     0.189     0.316\n5 OPS          0.112     0.316\n6 SORate      -0.0178    0.316\n7 age          0.367     0.316\n\n\n\npredictions_ridge_new &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name        Salary_ZScore    .pred  Year team_payroll    PA    BA    HR   age\n   &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 A. J. Ellis       -0.765   3.05e-5  2012 top            505 0.27     13    31\n 2 A. J. Pier…       -0.922  -7.82e-1  2001 lower          405 0.289     7    24\n 3 A. J. Pier…       -0.938  -6.30e-1  2002 lower          469 0.3       6    25\n 4 A. J. Pier…       -0.342  -2.24e-1  2005 upper          497 0.257    18    28\n 5 A. J. Pier…       -0.0227 -4.24e-2  2006 upper          543 0.295    16    29\n 6 A. J. Pier…        0.217  -1.11e-1  2007 upper          509 0.263    14    30\n 7 Aaron Boone       -0.868  -2.46e-1  2001 middle         427 0.294    14    28\n 8 Aaron Hill         0.0345 -5.18e-1  2011 middle         429 0.225     6    29\n 9 Aaron Hill         0.146   5.14e-1  2012 lower          668 0.302    26    30\n10 Aaron Rowa…       -0.945  -9.35e-1  2002 upper          331 0.258     7    25\n# ℹ 718 more rows\n# ℹ 21 more variables: salary &lt;int&gt;, R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;,\n#   OBP &lt;dbl&gt;, BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name        Salary_ZScore    .pred  Year team_payroll    PA    BA    HR   age\n   &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 A. J. Ellis       -0.765  -0.0613   2012 top            505 0.27     13    31\n 2 A. J. Pier…       -0.922  -0.826    2001 lower          405 0.289     7    24\n 3 A. J. Pier…       -0.938  -0.663    2002 lower          469 0.3       6    25\n 4 A. J. Pier…       -0.342  -0.152    2005 upper          497 0.257    18    28\n 5 A. J. Pier…       -0.0227 -0.00559  2006 upper          543 0.295    16    29\n 6 A. J. Pier…        0.217  -0.0466   2007 upper          509 0.263    14    30\n 7 Aaron Boone       -0.868  -0.298    2001 middle         427 0.294    14    28\n 8 Aaron Hill         0.0345 -0.435    2011 middle         429 0.225     6    29\n 9 Aaron Hill         0.146   0.628    2012 lower          668 0.302    26    30\n10 Aaron Rowa…       -0.945  -1.00     2002 upper          331 0.258     7    25\n# ℹ 718 more rows\n# ℹ 21 more variables: salary &lt;int&gt;, R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;,\n#   OBP &lt;dbl&gt;, BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.806\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.799\n\n\n\npredictions_ridge_new_old &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_new_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adam…       -0.951  -0.411   2000 top            641 0.266     9    24 2.02e5\n 2 Adri…       -0.870  -0.714   1999 top            614 0.275    15    20 2.20e5\n 3 Adri…       -0.922  -0.665   2000 bottom         340 0.315     4    26 2.85e5\n 4 Al C…       -0.413   0.281   1985 upper          487 0.265    14    33 4   e5\n 5 Al M…        0.0937  0.419   1999 bottom         593 0.277    24    31 2.70e6\n 6 Al N…       -0.877  -0.332   1989 lower          521 0.253     0    29 1.80e5\n 7 Alan…        0.895   0.0777  1991 upper          421 0.248     9    33 2.20e6\n 8 Albe…        3.39    1.31    1998 upper          706 0.328    49    32 1   e7\n 9 Alex…       -0.860  -0.498   1999 &lt;NA&gt;           329 0.3       8    27 2.45e5\n10 Alex…       -0.752   0.131   1996 upper          677 0.358    36    21 4.42e5\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adam …       -0.951  -0.283  2000 top            641 0.266     9    24 2.02e5\n 2 Adria…       -0.870  -0.686  1999 top            614 0.275    15    20 2.20e5\n 3 Adria…       -0.922  -0.721  2000 bottom         340 0.315     4    26 2.85e5\n 4 Al Co…       -0.413   0.383  1985 upper          487 0.265    14    33 4   e5\n 5 Al Ma…        0.0937  0.449  1999 bottom         593 0.277    24    31 2.70e6\n 6 Al Ne…       -0.877  -0.219  1989 lower          521 0.253     0    29 1.80e5\n 7 Alan …        0.895   0.201  1991 upper          421 0.248     9    33 2.20e6\n 8 Alber…        3.39    1.50   1998 upper          706 0.328    49    32 1   e7\n 9 Alex …       -0.860  -0.569  1999 &lt;NA&gt;           329 0.3       8    27 2.45e5\n10 Alex …       -0.752   0.177  1996 upper          677 0.358    36    21 4.42e5\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.711\n\n\nLastly, what I did was compare the old model to the new model. We did this by plugging the test_old into both models to see which model predicted a higher value and we also plugged the test_new into both models to see which model predicted the higher values. As it turned out, there was a difference between the models and the older model predicted higher salary z-scores for both models indicating that the older players are predicted to be payed more than the new players thus showing that there is a significant pay disparity between the older group of players during the steroid era and the newer group of players.\nExamples of some of these numbers\nAlbert Pujoles 2002 (Old model prediction: 0.26) (New model prediction: 0.14) Albert Pujoles 2008 (Old model prediction: 0.79) (New model prediction: 0.74) Albert Pujoles 2009 (Old model prediction: 1.26) (New model prediction: 1.07) Albert Pujoles 2015 (Old model prediction: 1.29) (New model prediction: 0.99) Albert Pujoles 2016 (Old model prediction: 1.24) (New model prediction: 1.02)\nAlex Rodriguez 2003 (Old model prediction: 0.95) (New model prediction: 0.82) Alex Rodriguez 2006 (Old model prediction: 0.93) (New model prediction: 0.84) Alex Rodriguez 2008 (Old model prediction: 0.95) (New model prediction: 0.92)\nWhile this is merely to illustrate data that I saw using a basic non-mathematical eye test, the first thing that caught my eye was that I saw the old model was consistently higher than the new model for the top players in the league. I decided to construct a two-sided t-test to see if I could produce the model that I was looking for.\n\ngroupA&lt;-predictions_ridge_old_new$.pred\ngroupB&lt;-predictions_ridge_new$.pred\n\n\nt.test(groupA, groupB, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  groupA and groupB\nt = 0.89013, df = 1439.8, p-value = 0.3735\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03093645  0.08233683\nsample estimates:\n   mean of x    mean of y \n 0.009863549 -0.015836640 \n\n\n\ngroup1&lt;-predictions_ridge_old$.pred\ngroup2&lt;-predictions_ridge_new_old$.pred\n\nt.test(group1, group2, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 1.7948, df = 1325.1, p-value = 0.07291\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.0046900  0.1055498\nsample estimates:\n  mean of x   mean of y \n-0.01019777 -0.06062765 \n\n\nHowever, upon further inspection of the entire dataset and not just the outliers, we see in a two sample t.test that there is not enough evidence to support the alternative hypothesis that there is a difference of means and thus we conclude not only that the players in the old dataset and the new dataset have no significant difference in stats but adjusting for inflation no significant differences in salaries as well."
  },
  {
    "objectID": "Baseball.html#insights",
    "href": "Baseball.html#insights",
    "title": "Baseball",
    "section": "Insights",
    "text": "Insights\nI saw that the model did hint at a slight possibility that there was a marginal difference even though we cannot reject the null hypothesis. That being said, there are a few takeaways from this experiment that I gathered. One is that there is a significant difference in baseball between the top 5% of hitters and average league hitters. The salaries are so favored for the best players that they tend to see bigger number than the rest of the league anyway so it isn’t probably very useful to the league as a whole.\n\nLimitations and Future Work\nThe clear limitation to this project would be the overall limited-ness of the salary database. As mentioned previously, there was not a very large data set for salaries since they only went back to the 1980s.\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  }
]