[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Baseball Analytics",
    "section": "",
    "text": "Brian’s website https://websitename_name_html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Baseball.html",
    "href": "Baseball.html",
    "title": "Baseball",
    "section": "",
    "text": "Include that my baseball salaries only went back to 1980 and up to 2016, left out many key years."
  },
  {
    "objectID": "Baseball.html#exploratory-data-analysis",
    "href": "Baseball.html#exploratory-data-analysis",
    "title": "Baseball",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDefining my era’s\nThe first thing that I need to do is define my eras of baseball. Since the salary database only goes back to 1985, I divided my old and new eras into 15 year windows from 1985-2000 and 2001-2016\n\nlibrary(Lahman)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nsalaries &lt;- Salaries |&gt;\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People |&gt;\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() |&gt;\n  left_join(salaries, \n            by =c(\"playerID\", \"yearID\", \"teamID\")) |&gt;\n  left_join(peopleInfo, by = \"playerID\") |&gt;\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) |&gt;\n  arrange(playerID, yearID, stint)\n\nbatting_older &lt;- batting |&gt; filter(G &gt;= 100, AB &gt;= 200, 2001 &gt; yearID, yearID &gt; 1984) |&gt;\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_older &lt;- batting_older |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))"
  },
  {
    "objectID": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "href": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "title": "Baseball",
    "section": "Here I will modify the dataset a little bit to get the variables that I want.",
    "text": "Here I will modify the dataset a little bit to get the variables that I want.\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nSince a HR when successfully achieved in baseball also counts as an RBI and R, I decided to combine them into a single variable.\n\nCreating a single variable for HR, RBI, and R\nBelow, we will create the PC variable called PC_Score which will use principal component analysis to create an equation for all 3 variables and combine them into a single variable.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_XJeKD\n2 RBI    0.607 PC1       pca_XJeKD\n3 R      0.539 PC1       pca_XJeKD\n4 HR     0.518 PC2       pca_XJeKD\n5 RBI    0.232 PC2       pca_XJeKD\n6 R     -0.823 PC2       pca_XJeKD\n7 HR     0.625 PC3       pca_XJeKD\n8 RBI   -0.760 PC3       pca_XJeKD\n9 R      0.178 PC3       pca_XJeKD\n\n\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))"
  },
  {
    "objectID": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "href": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "title": "Baseball",
    "section": "Here, we now add the PC_Score variable to our dataset",
    "text": "Here, we now add the PC_Score variable to our dataset\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow, I will take the salary variable and turn it into a z-score to account for the change in salary due to inflation. i.e. I don’t want inflation to undermine what I am trying to accomplish comparing eras because an increase in nominal salary could easily just be a function of inflation of the dollar and nothing related to the skill of the player.\n\nbat_old &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_XJeKD\n2 RBI    0.607 PC1       pca_XJeKD\n3 R      0.539 PC1       pca_XJeKD\n4 HR     0.518 PC2       pca_XJeKD\n5 RBI    0.232 PC2       pca_XJeKD\n6 R     -0.823 PC2       pca_XJeKD\n7 HR     0.625 PC3       pca_XJeKD\n8 RBI   -0.760 PC3       pca_XJeKD\n9 R      0.178 PC3       pca_XJeKD\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\nThen repeating the process of creating a data set and PC_Score for the new era of players.\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nLooking at the top of the list and seeing the heaviest hitters Barry Bonds, Sammy Sosa, and Alex Rodriguez on the list for PC_Score gave me the idea that the list made sense. I also looked for a player like Ichiro, one of the most talented contact hitters, to see how he would be evaluated. Given that he was placed in the upper-middle of the pack, it made sense that he would not be featured with the heaviest sluggers in the league.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nBelow we are adding standard deviation of salary.\n\nbat_new &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_8iZkx\n2 RBI    0.604 PC1       pca_8iZkx\n3 R      0.544 PC1       pca_8iZkx\n4 HR    -0.531 PC2       pca_8iZkx\n5 RBI   -0.225 PC2       pca_8iZkx\n6 R      0.817 PC2       pca_8iZkx\n7 HR    -0.616 PC3       pca_8iZkx\n8 RBI    0.765 PC3       pca_8iZkx\n9 R     -0.189 PC3       pca_8iZkx\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\n\nWe are now going to adjust the Salary_ZScore values in our data frames to not include N/A values as values that we are interested in observing so that graphically, we won’t have any issues below.\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nNow, one of the questions that I wanted to explore using the eye test first was if there was a difference in any stat categories between the era’s that might account for the cause of a salary difference. I created the percentiles of some of the major stat categories below and compared them between the older and newer datasets.\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n3 metrics that I especially looked at when I wanted to compared these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being base on statistics but then I wonder about just the top 10%, 5%, 1% of players to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that I chose and there is no significant distinction between them just based on the eye test, so we can rule out different offensive numbers as a possible cause of any pay disparity.\nHere are some graphs of the percentiles above to visually show also that they do not appear to be very different from each other.\n\necdf(bat_old$BA)\n\nEmpirical CDF \nCall: ecdf(bat_old$BA)\n x[1:179] =  0.172,  0.179,  0.181,  ...,  0.379,  0.394\n\nplot(ecdf(bat_old$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$age)\n\nEmpirical CDF \nCall: ecdf(bat_old$age)\n x[1:26] =     19,     20,     21,  ...,     43,     44\n\nplot(ecdf(bat_old$age))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PA)\n\nEmpirical CDF \nCall: ecdf(bat_old$PA)\n x[1:511] =    214,    219,    221,  ...,    758,    773\n\nplot(ecdf(bat_old$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_old$PC_score)\n x[1:3266] = -2.9923, -2.8918, -2.8604,  ..., 6.3425, 6.4562\n\nplot(ecdf(bat_old$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_old$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_old$OPS)\n x[1:683] =   0.45,  0.465,  0.466,  ...,  1.216,  1.222\n\nplot(ecdf(bat_old$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_old$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_old$SORate)\n x[1:2991] = 0.025997, 0.027237, 0.02812,  ..., 0.36406, 0.38765\n\nplot(ecdf(bat_old$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_old$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_old$Salary_ZScore)\n x[1:2315] = -1.2453, -1.245, -1.2447,  ..., 3.6882, 3.7975\n\nplot(ecdf(bat_old$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_new$Salary_ZScore)\n x[1:2410] = -0.95118, -0.95065, -0.94986,  ..., 5.2009, 5.2309\n\nplot(ecdf(bat_new$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$BA)\n\nEmpirical CDF \nCall: ecdf(bat_new$BA)\n x[1:176] =  0.159,  0.174,  0.178,  ...,   0.37,  0.372\n\nplot(ecdf(bat_new$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PA)\n\nEmpirical CDF \nCall: ecdf(bat_new$PA)\n x[1:518] =    215,    221,    222,  ...,    765,    778\n\nplot(ecdf(bat_new$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_new$PC_score)\n x[1:3565] = -3.1599, -3.1129, -3.1032,  ..., 6.4042, 6.8327\n\nplot(ecdf(bat_new$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_new$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_new$SORate)\n x[1:3272] = 0.04246, 0.045455, 0.046791,  ..., 0.36239, 0.36565\n\nplot(ecdf(bat_new$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_new$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_new$OPS)\n x[1:661] =  0.488,  0.521,   0.53,  ...,  1.381,  1.421\n\nplot(ecdf(bat_new$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_new$age)\n\nEmpirical CDF \nCall: ecdf(bat_new$age)\n x[1:27] =     20,     21,     22,  ...,     46,     47\n\nplot(ecdf(bat_new$age))\n\n\n\n\n\n\n\n\nEXPERIMENT\nBriefly plotting the correlation matrix to see if there are any statistics that are worth looking at for similarity to combine into one variable led me to a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that I already have a heavy hitter stat for PC_score, I decided to keep these variables separate to account for this difference. For example, Ichiro was as dominant as he was due to his league leading 0.350+ BA but if you looked at OPS, we was about 0.82, which barely cracks the top 25% of players and he is most definitely a player that was in the top 5% of players in the league and a former league MVP. For this reason, I kept the two metrics separate.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nThe scatter plots below I do not believe are particularly relevant to the salary discussion because of how much noise there are in the graphs but to show that they do not really contribute to the salary discussion, I plotted them below to at least see the visual data.\n\nplot(bat_new$BA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$age, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$BA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$SORate, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$age)"
  },
  {
    "objectID": "Baseball.html#modeling",
    "href": "Baseball.html#modeling",
    "title": "Baseball",
    "section": "Modeling",
    "text": "Modeling\nNow to actually look at the salary variable in a ridge regression model, first the old and new data sets will be split into 4 data sets called test_old, test_new, train_old, and train_new.\n\nn&lt;-nrow(bat_old)\nm&lt;-nrow(bat_new)\n\nindices_old&lt;-sample(n, 0.8*floor(n), replace=F)\nindices_new&lt;-sample(m, 0.8*floor(m), replace=F)\n\ntest_old&lt;- bat_old[-indices_old, ]\ntrain_old&lt;- bat_old[indices_old, ]\n\ntest_new&lt;- bat_new[-indices_new, ]\ntrain_new &lt;- bat_new[indices_new, ]\n\nLastly, I decided to compute a ridge model, one for the old players and one for the new players to see if there was anything that could be observed for the trends of the salaries based on these two eras.\n\nridge_model_old &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_old &lt;- workflow() |&gt;\n  add_model(ridge_model_old)\n\n\nridge_recipe_old &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_old\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_old &lt;- ridge_wflow_old |&gt;\n  add_recipe(ridge_recipe_old)\n\n\nset.seed(1332)\nbatting_cv_old &lt;- vfold_cv(train_old, v = 10)\n\nridge_tune_old &lt;- tune_grid(ridge_model_old, \n                         ridge_recipe_old,\n                      resamples = batting_cv_old, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\n\nridge_tune_old |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nridge_best_old &lt;- ridge_tune_old |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_old\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.202 Preprocessor1_Model17\n\nridge_wflow_final_old &lt;- ridge_wflow_old |&gt;\n  finalize_workflow(parameters = ridge_best_old) \n\nridge_wflow_final_old &lt;- fit(ridge_wflow_final_old, data=train_old) \nridge_wflow_final_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 486.60\n2    6  0.35 443.40\n3    6  0.39 404.00\n4    6  0.42 368.10\n5    6  0.46 335.40\n6    6  0.51 305.60\n7    6  0.56 278.40\n8    6  0.61 253.70\n9    6  0.67 231.20\n10   6  0.73 210.60\n11   6  0.80 191.90\n12   6  0.88 174.90\n13   6  0.96 159.30\n14   6  1.06 145.20\n15   6  1.16 132.30\n16   6  1.27 120.50\n17   6  1.39 109.80\n18   6  1.52 100.10\n19   6  1.66  91.18\n20   6  1.82  83.08\n21   6  1.99  75.70\n22   6  2.17  68.97\n23   6  2.37  62.85\n24   6  2.59  57.26\n25   6  2.83  52.18\n26   6  3.09  47.54\n27   6  3.37  43.32\n28   6  3.67  39.47\n29   6  4.00  35.96\n30   6  4.35  32.77\n31   6  4.74  29.86\n32   6  5.15  27.20\n33   6  5.59  24.79\n34   6  6.07  22.59\n35   6  6.58  20.58\n36   6  7.12  18.75\n37   6  7.71  17.09\n38   6  8.33  15.57\n39   6  8.99  14.18\n40   6  9.69  12.92\n41   6 10.43  11.78\n42   6 11.21  10.73\n43   6 12.04   9.78\n44   6 12.90   8.91\n45   6 13.80   8.12\n46   6 14.74   7.40\n\n...\nand 54 more lines.\n\n\n\nridge_pred_check_old &lt;- ridge_wflow_final_old |&gt;\n  fit_resamples(\n    resamples = batting_cv_old,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_old, y_pred = predict(ridge_wflow_final_old, new_data = test_old))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nridge_fit_old &lt;- ridge_wflow_final_old |&gt;\n  fit(data = train_old)\nridge_fit_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 486.60\n2    6  0.35 443.40\n3    6  0.39 404.00\n4    6  0.42 368.10\n5    6  0.46 335.40\n6    6  0.51 305.60\n7    6  0.56 278.40\n8    6  0.61 253.70\n9    6  0.67 231.20\n10   6  0.73 210.60\n11   6  0.80 191.90\n12   6  0.88 174.90\n13   6  0.96 159.30\n14   6  1.06 145.20\n15   6  1.16 132.30\n16   6  1.27 120.50\n17   6  1.39 109.80\n18   6  1.52 100.10\n19   6  1.66  91.18\n20   6  1.82  83.08\n21   6  1.99  75.70\n22   6  2.17  68.97\n23   6  2.37  62.85\n24   6  2.59  57.26\n25   6  2.83  52.18\n26   6  3.09  47.54\n27   6  3.37  43.32\n28   6  3.67  39.47\n29   6  4.00  35.96\n30   6  4.35  32.77\n31   6  4.74  29.86\n32   6  5.15  27.20\n33   6  5.59  24.79\n34   6  6.07  22.59\n35   6  6.58  20.58\n36   6  7.12  18.75\n37   6  7.71  17.09\n38   6  8.33  15.57\n39   6  8.99  14.18\n40   6  9.69  12.92\n41   6 10.43  11.78\n42   6 11.21  10.73\n43   6 12.04   9.78\n44   6 12.90   8.91\n45   6 13.80   8.12\n46   6 14.74   7.40\n\n...\nand 54 more lines.\n\n\n\nridge_fit_old |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_old &lt;- ridge_fit_old |&gt;\n  broom::tidy()\nridge_coef_old\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -0.0122   0.202\n2 BA           -0.0754   0.202\n3 PA            0.161    0.202\n4 PC_score      0.237    0.202\n5 OPS           0.121    0.202\n6 SORate       -0.0836   0.202\n7 age           0.393    0.202\n\n\nThe ridge regression model has been completed below.\n\npredictions_ridge_old &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_old |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 667 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adri…      -0.870   -0.748   1999 top            614 0.275    15    20 2.20e5\n 2 Al M…      -0.00646 -0.131   1997 bottom         477 0.291    13    29 2.27e6\n 3 Al N…      -0.877   -0.231   1989 lower          521 0.253     0    29 1.80e5\n 4 Alan…       0.0736   0.725   1987 upper          668 0.343    28    29 6.17e5\n 5 Albe…       0.324    0.427   1994 bottom         480 0.357    36    28 2.78e6\n 6 Alex…      -1.09    -0.452   1994 lower          398 0.296     4    29 3.75e5\n 7 Alvi…       1.19     0.518   1989 upper          611 0.305    21    29 1.4 e6\n 8 Alvi…       0.437    0.0730  1991 upper          528 0.221    12    31 1.73e6\n 9 Andr…       1.21     0.758   1986 bottom         475 0.229    17    37 1.10e6\n10 Andr…      -1.11    -0.983   1986 middle         356 0.271    10    25 6.75e4\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.784\n\n\n\npredictions_ridge_old_new &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_old_new |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 728 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J.…       -0.809  -0.286  2003 lower          533 0.312    11    26 3.65e5\n 2 A. J.…       -0.0406 -0.193  2004 upper          510 0.272    11    27 3.5 e6\n 3 A. J.…       -0.524   0.278  2011 upper          500 0.287     8    34 2   e6\n 4 Aaron…       -0.453   0.458  2002 middle         685 0.241    26    29 2.10e6\n 5 Aaron…       -0.171   0.249  2005 bottom         565 0.243    16    32 3   e6\n 6 Aaron…       -0.147   0.123  2010 middle         580 0.205    26    28 4   e6\n 7 Aaron…        0.952   0.145  2008 upper          611 0.271    13    31 9.60e6\n 8 Aaron…        0.907   0.143  2009 upper          546 0.261    15    32 9.60e6\n 9 Abrah…       -0.624  -0.518  2006 top            369 0.211     2    30 1.13e6\n10 Adam …        0.709   0.280  2006 middle         683 0.234    40    26 7.5 e6\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.751\n\n\n\nridge_model_new &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_new &lt;- workflow() |&gt;\n  add_model(ridge_model_new)\n\n\nridge_recipe_new &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_new\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_new &lt;- ridge_wflow_new |&gt;\n  add_recipe(ridge_recipe_new)\n\n\nset.seed(1332)\nbatting_cv_new &lt;- vfold_cv(train_new, v = 10)\n\nridge_tune_new &lt;- tune_grid(ridge_model_new, \n                         ridge_recipe_new,\n                      resamples = batting_cv_new, \n                      grid = grid_regular(penalty(range = c(-1, 2.5)), levels = 50))\n\n\nridge_tune_new |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\nridge_best_new &lt;- ridge_tune_new |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_new\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.373 Preprocessor1_Model09\n\nridge_wflow_final_new &lt;- ridge_wflow_new |&gt;\n  finalize_workflow(parameters = ridge_best_new) \n\nridge_wflow_final_new &lt;- fit(ridge_wflow_final_new, data=train_new) \n\n\nridge_pred_check_new &lt;- ridge_wflow_final_new |&gt;\n  fit_resamples(\n    resamples = batting_cv_new,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_new, y_pred = predict(ridge_wflow_final_new, new_data = test_new))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nridge_fit_new &lt;- ridge_wflow_final_new |&gt;\n  fit(data = train_new)\nridge_fit_new\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_mean()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 464.60\n2    6  0.31 423.40\n3    6  0.34 385.70\n4    6  0.37 351.50\n5    6  0.41 320.30\n6    6  0.45 291.80\n7    6  0.49 265.90\n8    6  0.54 242.30\n9    6  0.59 220.70\n10   6  0.65 201.10\n11   6  0.71 183.30\n12   6  0.78 167.00\n13   6  0.85 152.10\n14   6  0.93 138.60\n15   6  1.02 126.30\n16   6  1.12 115.10\n17   6  1.22 104.90\n18   6  1.34  95.55\n19   6  1.47  87.06\n20   6  1.60  79.33\n21   6  1.75  72.28\n22   6  1.92  65.86\n23   6  2.09  60.01\n24   6  2.29  54.68\n25   6  2.50  49.82\n26   6  2.72  45.39\n27   6  2.97  41.36\n28   6  3.24  37.69\n29   6  3.53  34.34\n30   6  3.84  31.29\n31   6  4.18  28.51\n32   6  4.54  25.98\n33   6  4.93  23.67\n34   6  5.35  21.57\n35   6  5.80  19.65\n36   6  6.28  17.90\n37   6  6.79  16.31\n38   6  7.34  14.86\n39   6  7.92  13.54\n40   6  8.54  12.34\n41   6  9.19  11.24\n42   6  9.88  10.25\n43   6 10.60   9.34\n44   6 11.36   8.51\n45   6 12.15   7.75\n46   6 12.98   7.06\n\n...\nand 54 more lines.\n\n\n\nridge_fit_new |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_new &lt;- ridge_fit_new |&gt;\n  broom::tidy()\nridge_coef_new\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.00478   0.373\n2 BA          -0.0164    0.373\n3 PA           0.135     0.373\n4 PC_score     0.185     0.373\n5 OPS          0.103     0.373\n6 SORate      -0.0260    0.373\n7 age          0.350     0.373\n\n\n\npredictions_ridge_new &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…       -0.809  -0.251   2003 lower          533 0.312    11    26 3.65e5\n 2 A. J…       -0.0406 -0.280   2004 upper          510 0.272    11    27 3.5 e6\n 3 A. J…       -0.524   0.192   2011 upper          500 0.287     8    34 2   e6\n 4 Aaro…       -0.453   0.308   2002 middle         685 0.241    26    29 2.10e6\n 5 Aaro…       -0.171   0.163   2005 bottom         565 0.243    16    32 3   e6\n 6 Aaro…       -0.147  -0.0440  2010 middle         580 0.205    26    28 4   e6\n 7 Aaro…        0.952   0.159   2008 upper          611 0.271    13    31 9.60e6\n 8 Aaro…        0.907   0.167   2009 upper          546 0.261    15    32 9.60e6\n 9 Abra…       -0.624  -0.529   2006 top            369 0.211     2    30 1.13e6\n10 Adam…        0.709   0.240   2006 middle         683 0.234    40    26 7.5 e6\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 30\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J.…       -0.809  -0.286  2003 lower          533 0.312    11    26 3.65e5\n 2 A. J.…       -0.0406 -0.193  2004 upper          510 0.272    11    27 3.5 e6\n 3 A. J.…       -0.524   0.278  2011 upper          500 0.287     8    34 2   e6\n 4 Aaron…       -0.453   0.458  2002 middle         685 0.241    26    29 2.10e6\n 5 Aaron…       -0.171   0.249  2005 bottom         565 0.243    16    32 3   e6\n 6 Aaron…       -0.147   0.123  2010 middle         580 0.205    26    28 4   e6\n 7 Aaron…        0.952   0.145  2008 upper          611 0.271    13    31 9.60e6\n 8 Aaron…        0.907   0.143  2009 upper          546 0.261    15    32 9.60e6\n 9 Abrah…       -0.624  -0.518  2006 top            369 0.211     2    30 1.13e6\n10 Adam …        0.709   0.280  2006 middle         683 0.234    40    26 7.5 e6\n# ℹ 718 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.761\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.751\n\n\n\npredictions_ridge_new_old &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_new_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adri…      -0.870   -0.671   1999 top            614 0.275    15    20 2.20e5\n 2 Al M…      -0.00646 -0.0735  1997 bottom         477 0.291    13    29 2.27e6\n 3 Al N…      -0.877   -0.296   1989 lower          521 0.253     0    29 1.80e5\n 4 Alan…       0.0736   0.607   1987 upper          668 0.343    28    29 6.17e5\n 5 Albe…       0.324    0.462   1994 bottom         480 0.357    36    28 2.78e6\n 6 Alex…      -1.09    -0.347   1994 lower          398 0.296     4    29 3.75e5\n 7 Alvi…       1.19     0.394   1989 upper          611 0.305    21    29 1.4 e6\n 8 Alvi…       0.437   -0.0352  1991 upper          528 0.221    12    31 1.73e6\n 9 Andr…       1.21     0.574   1986 bottom         475 0.229    17    37 1.10e6\n10 Andr…      -1.11    -0.787   1986 middle         356 0.271    10    25 6.75e4\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\npredictions_ridge_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 667 × 30\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adri…      -0.870   -0.748   1999 top            614 0.275    15    20 2.20e5\n 2 Al M…      -0.00646 -0.131   1997 bottom         477 0.291    13    29 2.27e6\n 3 Al N…      -0.877   -0.231   1989 lower          521 0.253     0    29 1.80e5\n 4 Alan…       0.0736   0.725   1987 upper          668 0.343    28    29 6.17e5\n 5 Albe…       0.324    0.427   1994 bottom         480 0.357    36    28 2.78e6\n 6 Alex…      -1.09    -0.452   1994 lower          398 0.296     4    29 3.75e5\n 7 Alvi…       1.19     0.518   1989 upper          611 0.305    21    29 1.4 e6\n 8 Alvi…       0.437    0.0730  1991 upper          528 0.221    12    31 1.73e6\n 9 Andr…       1.21     0.758   1986 bottom         475 0.229    17    37 1.10e6\n10 Andr…      -1.11    -0.983   1986 middle         356 0.271    10    25 6.75e4\n# ℹ 657 more rows\n# ℹ 20 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;\n\nrmse(predictions_ridge_new_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.802\n\n\nLastly, what I did was compare the old model to the new model. We did this by plugging the test_old into both models to see which model predicted a higher value and we also plugged the test_new into both models to see which model predicted the higher values. As it turned out, there was a difference between the models and the older model predicted higher salary z-scores for both models indicating that the older players are predicted to be payed more than the new players thus showing that there is a significant pay disparity between the older group of players during the steroid era and the newer group of players.\nExamples of some of these numbers\nAlbert Pujoles 2002 (Old model prediction: 0.26) (New model prediction: 0.14) Albert Pujoles 2008 (Old model prediction: 0.79) (New model prediction: 0.74) Albert Pujoles 2009 (Old model prediction: 1.26) (New model prediction: 1.07) Albert Pujoles 2015 (Old model prediction: 1.29) (New model prediction: 0.99) Albert Pujoles 2016 (Old model prediction: 1.24) (New model prediction: 1.02)\nAlex Rodriguez 2003 (Old model prediction: 0.95) (New model prediction: 0.82) Alex Rodriguez 2006 (Old model prediction: 0.93) (New model prediction: 0.84) Alex Rodriguez 2008 (Old model prediction: 0.95) (New model prediction: 0.92)\nWhile this is merely to illustrate data that I saw using a basic non-mathematical eye test, the first thing that caught my eye was that I saw the old model was consistently higher than the new model for the top players in the league. I decided to construct a two-sided t-test to see if I could produce the model that I was looking for.\n\ngroupA&lt;-predictions_ridge_old_new$.pred\ngroupB&lt;-predictions_ridge_new$.pred\n\n\nt.test(groupA, groupB, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  groupA and groupB\nt = 0.1961, df = 1423.4, p-value = 0.8446\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.04918640  0.06011272\nsample estimates:\n   mean of x    mean of y \n0.0063386432 0.0008754821 \n\n\n\ngroup1&lt;-predictions_ridge_old$.pred\ngroup2&lt;-predictions_ridge_new_old$.pred\n\nt.test(group1, group2, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.7611, df = 1306.2, p-value = 0.4467\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03587251  0.08135077\nsample estimates:\n   mean of x    mean of y \n 0.001448035 -0.021291093 \n\n\nHowever, upon further inspection of the entire dataset and not just the outliers, we see in a two sample t.test that there is not enough evidence to support the alternative hypothesis that there is a difference of means and thus we conclude not only that the players in the old dataset and the new dataset have no significant difference in stats but adjusting for inflation no significant differences in salaries as well."
  },
  {
    "objectID": "Baseball.html#insights",
    "href": "Baseball.html#insights",
    "title": "Baseball",
    "section": "Insights",
    "text": "Insights\nI saw that the model did hint at a slight possibility that there was a marginal difference even though we cannot reject the null hypothesis. That being said, there are a few takeaways from this experiment that I gathered. One is that there is a significant difference in baseball between the top 5% of hitters and average league hitters. The salaries are so favored for the best players that they tend to see bigger number than the rest of the league anyway so it isn’t probably very useful to the league as a whole.\n\nLimitations and Future Work\nThe clear limitation to this project would be the overall limited-ness of the salary database. As mentioned previously, there was not a very large data set for salaries since they only went back to the 1980s.\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  }
]