[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Final Project",
    "section": "",
    "text": "Math 437"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Baseball.html",
    "href": "Baseball.html",
    "title": "Baseball",
    "section": "",
    "text": "Viewing baseball in recent years has produced so many incredible moments that I am naturally drawn to use baseball as subject matter for this project. I have watched the sport for a very long time, and my primary piece of curiosity comes from an extension of the HR. As it is known to any person remotely involved in baseball, the use of illegal performance enhancing anabolic steroids to make players stronger so that they can hit the ball further has been a part of the game for over 100 years when Babe Ruth used steroids derived from sheep to boost his home run numbers."
  },
  {
    "objectID": "Baseball.html#exploratory-data-analysis",
    "href": "Baseball.html#exploratory-data-analysis",
    "title": "Baseball",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nAfter downloading the tidy packages and my baseball data set, the first thing to do is define the eras of baseball. Since the salary database only goes back to 1985, the eras were divided into 15 year windows from 1985-2000 and 2001-2016. Since creating each dataset takes multiple steps, the 1985-2000 called ‘old’ will be defined first.\n\nsalaries &lt;- Salaries |&gt;\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People |&gt;\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() |&gt;\n  left_join(salaries, \n            by =c(\"playerID\", \"yearID\", \"teamID\")) |&gt;\n  left_join(peopleInfo, by = \"playerID\") |&gt;\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) |&gt;\n  arrange(playerID, yearID, stint)\n\nbatting_older &lt;- batting |&gt; filter(G &gt;= 100, AB &gt;= 200, 2001 &gt; yearID, yearID &gt; 1984) |&gt;\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_older &lt;- batting_older |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\nThe data set will need additional variables, so it was modified briefly to include these changes.\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nDue to the nature of how statistics are calculated in baseball, a HR when successfully achieved in baseball also counts as an RBI and R. Due to this overlapping nature of the statistic, HR, R, and RBI will be combined into a single variable, which will be called PC_Score using Principal Component Analysis.\nUsing the recipe function, we will first add the desired variables HR, R, and RBI.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\nThen, the recipe is prepped.\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\nThen, the recipe is baked.\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\nLastly using the tidy function, the PC values are loaded.\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_kc0lz\n2 RBI    0.607 PC1       pca_kc0lz\n3 R      0.539 PC1       pca_kc0lz\n4 HR     0.518 PC2       pca_kc0lz\n5 RBI    0.232 PC2       pca_kc0lz\n6 R     -0.823 PC2       pca_kc0lz\n7 HR     0.625 PC3       pca_kc0lz\n8 RBI   -0.760 PC3       pca_kc0lz\n9 R      0.178 PC3       pca_kc0lz\n\n\nThe formula is then derived for PC1, which will be changed in name to PC_Score.\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\nThen it is added to the data set and the values are posted below.\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))\n\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow that PC_Score is added to the data set, adding the salary variable will come next. Due to inflation, the salary variable will be standardized into a Salary_ZScore. Since the graph of the salary histogram distribution was not originally a normal distribution, I decided to change the salary variable to 1/(salary^(1/8)) by experimenting with different exponents of salary to find an exponent that would turn the bat_old_proto into something resembling a normal distribution, which in this case is (1/8).\n\nbat_old_proto_2 &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_new = 1/salary^(1/8) |&gt; as.vector()) |&gt;\n  ungroup()\nhist(bat_old_proto_2$Salary_new)\n\n\n\n\n\n\n\nbat_old &lt;- bat_old_proto_2 |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(Salary_new) |&gt; as.vector()) |&gt;\n  ungroup()\n\nThen repeating this process, the second data set for players from 2001-2016 are now included into a data set called ‘new’.\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\nModifying the variables.\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nCreating the variable for HR/RBI/R called PC_Score.\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\nPrepping and then baking the recipe.\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nAdding the variable to the dataset.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nStandardizing salary after changing Salary_new to 1/salary^(1/8)\n\nbat_new_proto_2 &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_new = 1/salary^(1/8) |&gt; as.vector()) |&gt;\n  ungroup()\nhist(bat_new_proto_2$Salary_new)\n\n\n\n\n\n\n\nbat_new &lt;- bat_new_proto_2 |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(Salary_new) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_sXNwf\n2 RBI    0.604 PC1       pca_sXNwf\n3 R      0.544 PC1       pca_sXNwf\n4 HR    -0.531 PC2       pca_sXNwf\n5 RBI   -0.225 PC2       pca_sXNwf\n6 R      0.817 PC2       pca_sXNwf\n7 HR    -0.616 PC3       pca_sXNwf\n8 RBI    0.765 PC3       pca_sXNwf\n9 R     -0.189 PC3       pca_sXNwf\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\nLastly, any N/A values in the variable Salary_ZScore will be dropped before continuing.\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nOne question that I wanted to explore before continuing was “Are the statistics different between the two group i.e. could salary be related to one of the era’s performing better than the other.”\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\npercentiles_BA_old\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.17200 0.22400 0.23500 0.24200 0.24700 0.25200 0.25600 0.26000 0.26400 0.26800 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.27100 0.27500 0.27880 0.28300 0.28700 0.29200 0.29700 0.30400 0.31100 0.32300 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.32600 0.33000 0.33600 0.34100 0.34900 0.35871 0.39400 \n\n\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\npercentile_BA_new\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.15900 0.22300 0.23300 0.24000 0.24600 0.25100 0.25500 0.25900 0.26300 0.26660 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.27000 0.27300 0.27700 0.28200 0.28500 0.28900 0.29400 0.30000 0.30600 0.31800 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.32100 0.32500 0.33000 0.33400 0.33800 0.34696 0.37200 \n\n\n\npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PA_old\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n214.00 313.00 349.00 380.00 407.60 431.00 450.00 470.00 489.00 511.00 527.00 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%    96%    97% \n547.00 568.00 587.00 606.00 625.00 641.00 660.00 677.00 696.00 701.00 709.00 \n   98%  98.5%    99%  99.5%   100% \n717.00 721.00 726.00 737.71 773.00 \n\n\n\npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\npercentile_PA_new\n\n    0%     5%    10%    15%    20%    25%    30%    35%    40%    45%    50% \n215.00 311.40 358.00 389.20 418.00 438.00 456.00 478.00 500.00 518.00 538.00 \n   55%    60%    65%    70%    75%    80%    85%    90%    95%    96%    97% \n560.00 578.00 596.00 614.00 632.00 649.00 665.00 681.00 701.00 706.00 712.00 \n   98%  98.5%    99%  99.5%   100% \n718.84 725.00 731.00 739.00 778.00 \n\n\n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PC_old\n\n         0%          5%         10%         15%         20%         25% \n-2.99231806 -2.16175833 -1.87079048 -1.61930016 -1.38543523 -1.20970247 \n        30%         35%         40%         45%         50%         55% \n-1.01192213 -0.82900679 -0.62997862 -0.44687802 -0.23470836 -0.04265409 \n        60%         65%         70%         75%         80%         85% \n 0.18084883  0.42937859  0.72775522  1.02080990  1.37207629  1.75342264 \n        90%         95%         96%         97%         98%       98.5% \n 2.18583078  2.89302182  3.13819967  3.38046443  3.73994642  3.94390479 \n        99%       99.5%        100% \n 4.21382564  4.57420554  6.45624855 \n\n\n\npercentiles_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_PC_new\n\n          0%           5%          10%          15%          20%          25% \n-3.159885159 -2.236617151 -1.904976234 -1.632923721 -1.389647682 -1.197464047 \n         30%          35%          40%          45%          50%          55% \n-0.986158344 -0.800025977 -0.596640550 -0.401215686 -0.217399848 -0.002404445 \n         60%          65%          70%          75%          80%          85% \n 0.234612085  0.487738521  0.736773063  1.008367577  1.327689814  1.733745438 \n         90%          95%          96%          97%          98%        98.5% \n 2.225818652  2.908677146  3.102227281  3.292356325  3.593860825  3.885859410 \n         99%        99.5%         100% \n 4.114345227  4.562982520  6.832739065 \n\n\n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_SORate_old\n\n        0%         5%        10%        15%        20%        25%        30% \n0.02599653 0.06809067 0.08259171 0.09310484 0.10099334 0.11001969 0.11730861 \n       35%        40%        45%        50%        55%        60%        65% \n0.12377639 0.12957403 0.13578392 0.14170040 0.14766230 0.15363483 0.16165850 \n       70%        75%        80%        85%        90%        95%        96% \n0.16866649 0.17689917 0.18550138 0.19829709 0.21331720 0.23486239 0.24086542 \n       97%        98%      98.5%        99%      99.5%       100% \n0.25079023 0.26288020 0.26998529 0.28194742 0.29799283 0.38765432 \n\n\n\npercentiles_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_SORate_new\n\n        0%         5%        10%        15%        20%        25%        30% \n0.04245974 0.08805424 0.10183586 0.11183144 0.12169312 0.12979890 0.13868421 \n       35%        40%        45%        50%        55%        60%        65% \n0.14614294 0.15322835 0.15976331 0.16608997 0.17353108 0.18151769 0.18917430 \n       70%        75%        80%        85%        90%        95%        96% \n0.19730342 0.20579268 0.21667851 0.22910250 0.24463240 0.27045781 0.27703585 \n       97%        98%      98.5%        99%      99.5%       100% \n0.28755084 0.30055950 0.30908875 0.32068440 0.33275529 0.36565097 \n\n\n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_OPS_old\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.45000 0.60490 0.63900 0.66000 0.67900 0.69400 0.70700 0.72100 0.73300 0.74700 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.76000 0.77200 0.78400 0.79800 0.81400 0.83200 0.85000 0.87500 0.90420 0.95710 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.97336 0.99400 1.02000 1.03213 1.05942 1.09042 1.22200 \n\n\n\npercentiles_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_OPS_new\n\n     0%      5%     10%     15%     20%     25%     30%     35%     40%     45% \n0.48800 0.62340 0.65400 0.67400 0.68900 0.70300 0.71600 0.72800 0.74100 0.75300 \n    50%     55%     60%     65%     70%     75%     80%     85%     90%     95% \n0.76500 0.77800 0.78800 0.80200 0.81500 0.83100 0.85100 0.87300 0.89900 0.94960 \n    96%     97%     98%   98.5%     99%   99.5%    100% \n0.96168 0.97976 1.00484 1.01688 1.04092 1.08384 1.42100 \n\n\n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_age_old\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n   19    23    24    25    26    26    27    27    28    28    29    29    30 \n  65%   70%   75%   80%   85%   90%   95%   96%   97%   98% 98.5%   99% 99.5% \n   30    31    31    32    33    34    36    37    37    38    39    39    40 \n 100% \n   44 \n\n\n\npercentiles_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\npercentiles_age_new\n\n   0%    5%   10%   15%   20%   25%   30%   35%   40%   45%   50%   55%   60% \n   19    23    24    25    26    26    27    27    28    28    29    29    30 \n  65%   70%   75%   80%   85%   90%   95%   96%   97%   98% 98.5%   99% 99.5% \n   31    31    32    33    34    35    36    37    37    38    39    40    40 \n 100% \n   47 \n\n\n3 metrics that were especially looked at when comparing these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being based on statistics but then pondering the top 10%, 5%, 1% of players, these values were also noted to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that were chosen and there is no significant distinction between them just based on the eye test i.e. better statistics can be ruled out as a possible cause of any pay disparity.\nLastly, the correlation matrix was created to see if there are any statistics that are worth looking at for similarity to combine into one variable similar to what was done to PC_Score. This led to finding a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that PC_score is already a heavy hitter stat, OPS and BA were not combined into a single variable.\nFor example, Ichiro, a dominant contact hitter with a league leading 0.350 BA, would be over looked if BA were omitted because as a power hitter, he was not nearly as powerful as the other players and he would get overlooked in the database if the league leading contact part of his game were omitted.\n\nlibrary(corrplot)\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\n\nplot(FLINGO$PC_score, FLINGO$PA)\n\n\n\n\n\n\n\nplot(FLINGO$OPS, FLINGO$BA)\n\n\n\n\n\n\n\nplot(FLINGO$PC_score, FLINGO$OPS)\n\n\n\n\n\n\n\nplot(FLINGO$PC_score, FLINGO$BA)\n\n\n\n\n\n\n\nplot(FLINGO$OPS, FLINGO$PA)\n\n\n\n\n\n\n\nplot(FLINGO$Salary_new, FLINGO$age)\n\n\n\n\n\n\n\nplot(FLINGO$BA, FLINGO$SORate)\n\n\n\n\n\n\n\n\nThe distribution for PC_score and PA is logarithmic and so a linear correlation plot would be misleading. The rest of the variables appear to be linear but there are non that I want to use as part of my ridge regression model because of the intel on the players that I would be lacking."
  },
  {
    "objectID": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "href": "Baseball.html#here-i-will-modify-the-dataset-a-little-bit-to-get-the-variables-that-i-want.",
    "title": "Baseball",
    "section": "Here I will modify the dataset a little bit to get the variables that I want.",
    "text": "Here I will modify the dataset a little bit to get the variables that I want.\n\nbat_old_proto &lt;- batting_older |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nSince a HR when successfully achieved in baseball also counts as an RBI and R, I decided to combine them into a single variable.\n\nCreating a single variable for HR, RBI, and R\nBelow, we will create the PC variable called PC_Score which will use principal component analysis to create an equation for all 3 variables and combine them into a single variable.\n\npca_recipe &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_old_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\npca_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3455 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\n\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\npca_baked\n\n# A tibble: 3,455 × 5\n   Name            Year     PC1     PC2     PC3\n   &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Kurt Abbott     1994 -1.51    0.345   0.335 \n 2 Kurt Abbott     1995 -0.0500  0.319   0.192 \n 3 Kurt Abbott     1996 -1.65    0.432   0.247 \n 4 Bobby Abreu     1998  0.438   0.167  -0.138 \n 5 Bobby Abreu     1999  2.13   -1.22   -0.124 \n 6 Bobby Abreu     2000  1.76   -0.588   0.449 \n 7 Benny Agbayani  1999 -1.01    0.631   0.385 \n 8 Benny Agbayani  2000 -0.182   0.256   0.0678\n 9 Mike Aldrete    1987 -0.911   0.193  -0.0998\n10 Mike Aldrete    1988 -1.40    0.0965 -0.468 \n# ℹ 3,445 more rows\n\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_bkMMo\n2 RBI    0.607 PC1       pca_bkMMo\n3 R      0.539 PC1       pca_bkMMo\n4 HR     0.518 PC2       pca_bkMMo\n5 RBI    0.232 PC2       pca_bkMMo\n6 R     -0.823 PC2       pca_bkMMo\n7 HR     0.625 PC3       pca_bkMMo\n8 RBI   -0.760 PC3       pca_bkMMo\n9 R      0.178 PC3       pca_bkMMo\n\n\n\nbat_old_proto &lt;- bat_old_proto |&gt; \n  merge(pca_baked, \n            by =c(\"Name\", \"Year\"))"
  },
  {
    "objectID": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "href": "Baseball.html#here-we-now-add-the-pc_score-variable-to-our-dataset",
    "title": "Baseball",
    "section": "Here, we now add the PC_Score variable to our dataset",
    "text": "Here, we now add the PC_Score variable to our dataset\n\nbat_old_proto &lt;- bat_old_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nNow, I will take the salary variable and turn it into a z-score to account for the change in salary due to inflation. i.e. I don’t want inflation to undermine what I am trying to accomplish comparing eras because an increase in nominal salary could easily just be a function of inflation of the dollar and nothing related to the skill of the player.\n\nbat_old &lt;- bat_old_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\n\npca_tidy &lt;- tidy(pca_prep, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.584 PC1       pca_bkMMo\n2 RBI    0.607 PC1       pca_bkMMo\n3 R      0.539 PC1       pca_bkMMo\n4 HR     0.518 PC2       pca_bkMMo\n5 RBI    0.232 PC2       pca_bkMMo\n6 R     -0.823 PC2       pca_bkMMo\n7 HR     0.625 PC3       pca_bkMMo\n8 RBI   -0.760 PC3       pca_bkMMo\n9 R      0.178 PC3       pca_bkMMo\n\npca_loadings &lt;- pca_tidy |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.607  0.232 -0.760\n2 HR    0.584  0.518  0.625\n3 R     0.539 -0.823  0.178\n\narrange(pca_loadings, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.539 -0.823  0.178\n2 HR    0.584  0.518  0.625\n3 RBI   0.607  0.232 -0.760\n\n\nThen repeating the process of creating a data set and PC_Score for the new era of players.\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nsalaries &lt;- Salaries %&gt;%\n  dplyr::select(playerID, yearID, teamID, salary)\npeopleInfo &lt;- People %&gt;%\n  dplyr::select(playerID, birthYear, birthMonth, nameLast,\n         nameFirst, bats)\nbatting &lt;- battingStats() %&gt;% \n  left_join(salaries,\n            by =c(\"playerID\", \"yearID\", \"teamID\")) %&gt;%\n  left_join(peopleInfo, by = \"playerID\") %&gt;%\n  mutate(age = yearID - birthYear - \n           1L *(birthMonth &gt;= 10)) %&gt;%\n  arrange(playerID, yearID, stint)\n\nbatting_new &lt;- batting %&gt;% filter(G &gt;= 100, AB &gt;= 200, 2017 &gt; yearID, yearID &gt; 2000) %&gt;%\n  mutate(lgID = factor(lgID, levels = c(\"AL\", \"NL\"))) # fix the defunct league issue\n\nbatting_new &lt;- batting_new |&gt;\n  mutate(team_payroll = case_when(\n    teamID %in% c(\"ANA\", \"BOS\", \"CAL\", \"LAN\", \"NYA\", \"NYN\", \"PHI\", \"LAA\") ~ \"top\",\n    teamID %in% c(\"CHA\", \"CHN\", \"DET\", \"SEA\", \"SFN\", \"SLN\") ~ \"upper\",\n    teamID %in% c(\"ATL\", \"CIN\", \"HOU\", \"MON\", \"TEX\", \"TOR\") ~ \"middle\",\n    teamID %in% c(\"ARI\", \"BAL\", \"COL\", \"MIN\", \"ML4\", \"SDN\") ~ \"lower\",\n    teamID %in% c(\"CLE\", \"FLO\", \"KCA\", \"OAK\", \"PIT\", \"TBA\") ~ \"bottom\",\n    TRUE ~ NA_character_  # Optional: catch all others as NA\n  ))\n\n\nbat_new_proto &lt;- batting_new |&gt;\n  transmute(\n    Name = paste(nameFirst, nameLast),\n    Year = yearID,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    R = R,\n    RBI = RBI,\n    age=age,\n    salary=salary,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BB/PA, # walks per time at the plate\n    SORate = SO/PA, # strikeouts per time at the plate\n    DoubleRate = X2B/PA,\n    TripleRate = X3B/PA,\n    HRRate = HR/PA,\n    RRate = R/PA,\n    RBIRate = RBI/PA,\n    SBRate = SB/PA,\n    SBSuccessRate = SB/(pmax(SB + CS, 1)), # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\n\npca_recipe_new &lt;- recipe(\n  ~ Name + Year + HR + RBI + R, data = bat_new_proto\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors\n  update_role(c(Name, Year), new_role = \"id\") |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 10)\n\n\npca_prep_new &lt;- pca_recipe_new |&gt;\n  prep()\npca_prep_new\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\nid:        2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3799 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: HR, RBI, R | Trained\n\n\n• Dummy variables from: &lt;none&gt; | Trained\n\n\n• PCA extraction with: HR, RBI, R | Trained\n\npca_baked_new &lt;- pca_prep_new |&gt;\n  bake(new_data = NULL)\npca_baked_new\n\n# A tibble: 3,799 × 5\n   Name                Year   PC1     PC2     PC3\n   &lt;chr&gt;              &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Reggie Abercrombie  2006 -2.18 -0.0632 -0.303 \n 2 Brent Abernathy     2002 -1.81  0.198   0.292 \n 3 Bobby Abreu         2001  3.15  0.636   0.0449\n 4 Bobby Abreu         2002  1.57  0.855   0.0920\n 5 Bobby Abreu         2003  1.87  0.610   0.589 \n 6 Bobby Abreu         2004  2.98  0.731  -0.0433\n 7 Bobby Abreu         2005  2.24  0.572   0.340 \n 8 Bobby Abreu         2007  2.21  1.66    0.632 \n 9 Bobby Abreu         2008  1.87  0.654   0.552 \n10 Bobby Abreu         2009  1.57  0.743   0.971 \n# ℹ 3,789 more rows\n\nbat_new_proto &lt;- bat_new_proto |&gt; \n  merge(pca_baked_new, \n            by =c(\"Name\", \"Year\"))\n\nLooking at the top of the list and seeing the heaviest hitters Barry Bonds, Sammy Sosa, and Alex Rodriguez on the list for PC_Score gave me the idea that the list made sense. I also looked for a player like Ichiro, one of the most talented contact hitters, to see how he would be evaluated. Given that he was placed in the upper-middle of the pack, it made sense that he would not be featured with the heaviest sluggers in the league.\n\nbat_new_proto &lt;- bat_new_proto |&gt;\n  transmute(\n    Name = Name,\n    Year = Year,\n    team_payroll = team_payroll,\n    PA = PA,\n    BA = BA,\n    HR = HR,\n    age=age,\n    salary=salary,\n    R = R,\n    RBI = RBI,\n    SlugPct = SlugPct,\n    OBP = OBP, # these 3 are already rate stats\n    BBRate = BBRate, # walks per time at the plate\n    SORate = SORate, # strikeouts per time at the plate\n    DoubleRate = DoubleRate,\n    TripleRate = TripleRate,\n    HRRate = HRRate,\n    RRate = RRate,\n    RBIRate = RBIRate,\n    SBRate = SBRate,\n    SBSuccessRate = SBSuccessRate,\n    PC_score=PC1,\n    PC1=PC1,\n    PC2=PC2,\n    PC3=PC3,\n    OPS=OBP + SlugPct,\n    # to avoid NaN\n    bats,\n    lgID # just to have some categorical variables here\n  )\n\nBelow we are adding standard deviation of salary.\n\nbat_new &lt;- bat_new_proto |&gt;\n  group_by(Year) |&gt;\n  mutate(Salary_ZScore = scale(salary) |&gt; as.vector()) |&gt;\n  ungroup()\n\npca_tidy_new &lt;- tidy(pca_prep_new, 3, type = \"coef\") # tidy step 3 - the PCA step\nhead(pca_tidy_new, 20)\n\n# A tibble: 9 × 4\n  terms  value component id       \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n1 HR     0.582 PC1       pca_Wo635\n2 RBI    0.604 PC1       pca_Wo635\n3 R      0.544 PC1       pca_Wo635\n4 HR    -0.531 PC2       pca_Wo635\n5 RBI   -0.225 PC2       pca_Wo635\n6 R      0.817 PC2       pca_Wo635\n7 HR    -0.616 PC3       pca_Wo635\n8 RBI    0.765 PC3       pca_Wo635\n9 R     -0.189 PC3       pca_Wo635\n\npca_loadings_new &lt;- pca_tidy_new |&gt;\n  pivot_wider(names_from = \"component\",\n              values_from = \"value\") |&gt;\n  dplyr::select(!id)\narrange(pca_loadings_new, desc(abs(PC1)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 RBI   0.604 -0.225  0.765\n2 HR    0.582 -0.531 -0.616\n3 R     0.544  0.817 -0.189\n\narrange(pca_loadings_new, desc(abs(PC2)))\n\n# A tibble: 3 × 4\n  terms   PC1    PC2    PC3\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 R     0.544  0.817 -0.189\n2 HR    0.582 -0.531 -0.616\n3 RBI   0.604 -0.225  0.765\n\n\n\nWe are now going to adjust the Salary_ZScore values in our data frames to not include N/A values as values that we are interested in observing so that graphically, we won’t have any issues below.\n\nbat_old &lt;- bat_old |&gt;\n  drop_na(Salary_ZScore)\n\nbat_new &lt;- bat_new |&gt;\n  drop_na(Salary_ZScore)\n\nNow, one of the questions that I wanted to explore using the eye test first was if there was a difference in any stat categories between the era’s that might account for the cause of a salary difference. I created the percentiles of some of the major stat categories below and compared them between the older and newer datasets.\n\npercentiles_BA_old&lt;-quantile(bat_old_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentile_BA_new&lt;-quantile(bat_new_proto$BA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PA_old&lt;-quantile(bat_old_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentile_PA_new&lt;-quantile(bat_new_proto$PA, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1))\n\n\npercentiles_PC_old&lt;-quantile(bat_old_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_PC_new&lt;-quantile(bat_new_proto$PC_score, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_old&lt;-quantile(bat_old_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_SORate_new&lt;-quantile(bat_new_proto$SORate, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_old&lt;-quantile(bat_old_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_OPS_new&lt;-quantile(bat_new_proto$OPS, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_old&lt;-quantile(bat_old_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n\npercentiles_age_new&lt;-quantile(bat_new_proto$age, probs = c(0, 0.05, 0.1, 0.15, .2, .25, .3, .35, .4, .45, .5, .55, .6,\n                                                      .65, .7, .75, .8, .85, .9, .95, .96, .97, .98, .985, .99, .995, 1)) \n\n3 metrics that I especially looked at when I wanted to compared these eras were the 25%/50%/75% percentiles of PC_Score, BA, and OPS just to eyeball any significant difference in the offensive numbers of the players.\nOld BA 25%: 0.252, 50%: 0.271, 75%: 0.292 New BA 25%: 0.251, 50%: 0.270, 75%: 0.289\nOld OPS 25%: 0.694, 50%: 0.760, 75%: 0.832 New OPS 25%: 0.703, 50%: 0.765, 75%: 0.831\nOld PC_Score 25%: -1.20, 50%: -0.217, 75%: 1.01 New PC_Score 25%: -1.21, 50%: -0.235, 75%: 1.02\nThe eras are posting similar offensive numbers in 3 significant statistical categories so this can rule out any possible pay disparity over performance being base on statistics but then I wonder about just the top 10%, 5%, 1% of players to see if there is anything interesting with the outliers.\nOld BA 90%: 0.311, 95%: 0.323, 99%: 0.349 New BA 90%: 0.316, 95%: 0.318, 99%: 0.338\nOld OPS 90%: 0.904, 95%: 0.957, 99%: 1.06 New OPS 90%: 0.899, 95%: 0.950, 99%: 1.04\nOld PC_Score 90%: 2.19, 95%: 2.89, 99%: 4.21 New PC_Score 90%: 2.23, 95%: 2.91, 99%: 4.11\nThese numbers are very consistent across the board with all the percentiles that I chose and there is no significant distinction between them just based on the eye test, so we can rule out different offensive numbers as a possible cause of any pay disparity.\nHere are some graphs of the percentiles above to visually show also that they do not appear to be very different from each other.\n\necdf(bat_old$BA)\n\nEmpirical CDF \nCall: ecdf(bat_old$BA)\n x[1:179] =  0.172,  0.179,  0.181,  ...,  0.379,  0.394\n\nplot(ecdf(bat_old$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$age)\n\nEmpirical CDF \nCall: ecdf(bat_old$age)\n x[1:26] =     19,     20,     21,  ...,     43,     44\n\nplot(ecdf(bat_old$age))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PA)\n\nEmpirical CDF \nCall: ecdf(bat_old$PA)\n x[1:511] =    214,    219,    221,  ...,    758,    773\n\nplot(ecdf(bat_old$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_old$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_old$PC_score)\n x[1:3266] = -2.9923, -2.8918, -2.8604,  ..., 6.3425, 6.4562\n\nplot(ecdf(bat_old$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_old$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_old$OPS)\n x[1:683] =   0.45,  0.465,  0.466,  ...,  1.216,  1.222\n\nplot(ecdf(bat_old$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_old$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_old$SORate)\n x[1:2991] = 0.025997, 0.027237, 0.02812,  ..., 0.36406, 0.38765\n\nplot(ecdf(bat_old$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_old$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_old$Salary_ZScore)\n x[1:2315] = -1.2453, -1.245, -1.2447,  ..., 3.6882, 3.7975\n\nplot(ecdf(bat_old$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$Salary_ZScore)\n\nEmpirical CDF \nCall: ecdf(bat_new$Salary_ZScore)\n x[1:2410] = -0.95118, -0.95065, -0.94986,  ..., 5.2009, 5.2309\n\nplot(ecdf(bat_new$Salary_ZScore))\n\n\n\n\n\n\n\n\n\necdf(bat_new$BA)\n\nEmpirical CDF \nCall: ecdf(bat_new$BA)\n x[1:176] =  0.159,  0.174,  0.178,  ...,   0.37,  0.372\n\nplot(ecdf(bat_new$BA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PA)\n\nEmpirical CDF \nCall: ecdf(bat_new$PA)\n x[1:518] =    215,    221,    222,  ...,    765,    778\n\nplot(ecdf(bat_new$PA))\n\n\n\n\n\n\n\n\n\necdf(bat_new$PC_score)\n\nEmpirical CDF \nCall: ecdf(bat_new$PC_score)\n x[1:3565] = -3.1599, -3.1129, -3.1032,  ..., 6.4042, 6.8327\n\nplot(ecdf(bat_new$PC_score))\n\n\n\n\n\n\n\n\n\necdf(bat_new$SORate)\n\nEmpirical CDF \nCall: ecdf(bat_new$SORate)\n x[1:3272] = 0.04246, 0.045455, 0.046791,  ..., 0.36239, 0.36565\n\nplot(ecdf(bat_new$SORate))\n\n\n\n\n\n\n\n\n\necdf(bat_new$OPS)\n\nEmpirical CDF \nCall: ecdf(bat_new$OPS)\n x[1:661] =  0.488,  0.521,   0.53,  ...,  1.381,  1.421\n\nplot(ecdf(bat_new$OPS))\n\n\n\n\n\n\n\n\n\necdf(bat_new$age)\n\nEmpirical CDF \nCall: ecdf(bat_new$age)\n x[1:27] =     20,     21,     22,  ...,     46,     47\n\nplot(ecdf(bat_new$age))\n\n\n\n\n\n\n\n\nEXPERIMENT\nBriefly plotting the correlation matrix to see if there are any statistics that are worth looking at for similarity to combine into one variable led me to a correlation between OPS and BA, but OPS is a stat that favors heavy hitters and BA features great contact hitters. To make the distinction between these two types of players and the fact that I already have a heavy hitter stat for PC_score, I decided to keep these variables separate to account for this difference. For example, Ichiro was as dominant as he was due to his league leading 0.350+ BA but if you looked at OPS, we was about 0.82, which barely cracks the top 25% of players and he is most definitely a player that was in the top 5% of players in the league and a former league MVP. For this reason, I kept the two metrics separate.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nFLINGO&lt;-bat_new[,c(4,5,7,14,22,26,29)]\nZINGO&lt;-bat_old[,c(4,5,7,14,22,26,29)]\n\nBLINGO&lt;-cor(ZINGO)\nBINGO&lt;-cor(FLINGO)\n\ncorrplot(BINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\ncorrplot(BLINGO, method = \"circle\", type = \"upper\",\n         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nThe scatter plots below I do not believe are particularly relevant to the salary discussion because of how much noise there are in the graphs but to show that they do not really contribute to the salary discussion, I plotted them below to at least see the visual data.\n\nplot(bat_new$BA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$age, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$Salary_ZScore)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$BA)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$BA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$BA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$PC_Score)\n\nWarning: Unknown or uninitialised column: `PC_Score`.\n\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PA, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PA, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$OPS)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$PC_score, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$PC_score, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$SORate)\n\n\n\n\n\n\n\n\n\nplot(bat_old$OPS, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$OPS, bat_new$age)\n\n\n\n\n\n\n\n\n\nplot(bat_old$SORate, bat_old$age)\n\n\n\n\n\n\n\n\n\nplot(bat_new$SORate, bat_new$age)"
  },
  {
    "objectID": "Baseball.html#modeling",
    "href": "Baseball.html#modeling",
    "title": "Baseball",
    "section": "Modeling",
    "text": "Modeling\nNow to actually look at the salary variable in a ridge regression model, first the old and new data sets will be split into 4 data sets called test_old, test_new, train_old, and train_new.\n\nn&lt;-nrow(bat_old)\nm&lt;-nrow(bat_new)\n\nindices_old&lt;-sample(n, 0.8*floor(n), replace=F)\nindices_new&lt;-sample(m, 0.8*floor(m), replace=F)\n\ntest_old&lt;- bat_old[-indices_old, ]\ntrain_old&lt;- bat_old[indices_old, ]\n\ntest_new&lt;- bat_new[-indices_new, ]\ntrain_new &lt;- bat_new[indices_new, ]\n\nFirst, the ridge model for the old data set is created below.\nHow Does Ridge Regression Work?\nRegression creates a regularized linear model for the target variable of interest and it will do this in order to determine whether or not there is a significant squared error term. If the square error term is significant, ridge regression will provide a penalty to that variable’s coefficient potentially shrinking it close to zero or removing it all together. This way only variables that are seen as reliable and accurate are used in the construction of our model.\nRMSE is used instead of other squared errors because it is in the same units as y making it more in alignment with the variable of interest.\n\nridge_model_old &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_old &lt;- workflow() |&gt;\n  add_model(ridge_model_old)\n\nThe desired variables are then added into the recipe.\n\nridge_recipe_old &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_old\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_old &lt;- ridge_wflow_old |&gt;\n  add_recipe(ridge_recipe_old)\n\nThen, the recipe is cross validated and tuned.\n\nset.seed(1332)\nbatting_cv_old &lt;- vfold_cv(train_old, v = 10)\n\nridge_tune_old &lt;- tune_grid(ridge_model_old, \n                         ridge_recipe_old,\n                      resamples = batting_cv_old, \n                      grid = grid_regular(penalty(range = c(-2, 2)), levels = 50))\n\nThe desired tuning parameters are pulled from the model.\n\nridge_tune_old |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nThe model is then finalized.\n\nridge_best_old &lt;- ridge_tune_old |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_old\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.168 Preprocessor1_Model16\n\nridge_wflow_final_old &lt;- ridge_wflow_old |&gt;\n  finalize_workflow(parameters = ridge_best_old) \n\nridge_wflow_final_old &lt;- fit(ridge_wflow_final_old, data=train_old) \nridge_wflow_final_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 596.10\n2    6  0.30 543.20\n3    6  0.33 494.90\n4    6  0.36 450.90\n5    6  0.39 410.90\n6    6  0.43 374.40\n7    6  0.47 341.10\n8    6  0.52 310.80\n9    6  0.57 283.20\n10   6  0.62 258.00\n11   6  0.68 235.10\n12   6  0.75 214.20\n13   6  0.82 195.20\n14   6  0.90 177.90\n15   6  0.99 162.10\n16   6  1.08 147.70\n17   6  1.18 134.50\n18   6  1.30 122.60\n19   6  1.42 111.70\n20   6  1.55 101.80\n21   6  1.70  92.74\n22   6  1.86  84.50\n23   6  2.03  76.99\n24   6  2.22  70.15\n25   6  2.43  63.92\n26   6  2.66  58.24\n27   6  2.90  53.07\n28   6  3.17  48.35\n29   6  3.46  44.06\n30   6  3.77  40.14\n31   6  4.12  36.58\n32   6  4.48  33.33\n33   6  4.88  30.37\n34   6  5.31  27.67\n35   6  5.78  25.21\n36   6  6.28  22.97\n37   6  6.82  20.93\n38   6  7.39  19.07\n39   6  8.01  17.38\n40   6  8.67  15.83\n41   6  9.38  14.43\n42   6 10.13  13.15\n43   6 10.93  11.98\n44   6 11.77  10.91\n45   6 12.66   9.94\n46   6 13.60   9.06\n\n...\nand 54 more lines.\n\n\nThe predictions are made and a calibration plot is made to make sure that the model works.\n\nridge_pred_check_old &lt;- ridge_wflow_final_old |&gt;\n  fit_resamples(\n    resamples = batting_cv_old,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_old, y_pred = predict(ridge_wflow_final_old, new_data = test_old))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow, the finals pieces of the model are computed.\n\nridge_fit_old &lt;- ridge_wflow_final_old |&gt;\n  fit(data = train_old)\nridge_fit_old\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_impute_mean()\n• step_zv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 596.10\n2    6  0.30 543.20\n3    6  0.33 494.90\n4    6  0.36 450.90\n5    6  0.39 410.90\n6    6  0.43 374.40\n7    6  0.47 341.10\n8    6  0.52 310.80\n9    6  0.57 283.20\n10   6  0.62 258.00\n11   6  0.68 235.10\n12   6  0.75 214.20\n13   6  0.82 195.20\n14   6  0.90 177.90\n15   6  0.99 162.10\n16   6  1.08 147.70\n17   6  1.18 134.50\n18   6  1.30 122.60\n19   6  1.42 111.70\n20   6  1.55 101.80\n21   6  1.70  92.74\n22   6  1.86  84.50\n23   6  2.03  76.99\n24   6  2.22  70.15\n25   6  2.43  63.92\n26   6  2.66  58.24\n27   6  2.90  53.07\n28   6  3.17  48.35\n29   6  3.46  44.06\n30   6  3.77  40.14\n31   6  4.12  36.58\n32   6  4.48  33.33\n33   6  4.88  30.37\n34   6  5.31  27.67\n35   6  5.78  25.21\n36   6  6.28  22.97\n37   6  6.82  20.93\n38   6  7.39  19.07\n39   6  8.01  17.38\n40   6  8.67  15.83\n41   6  9.38  14.43\n42   6 10.13  13.15\n43   6 10.93  11.98\n44   6 11.77  10.91\n45   6 12.66   9.94\n46   6 13.60   9.06\n\n...\nand 54 more lines.\n\n\n\nridge_fit_old |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nridge_coef_old &lt;- ridge_fit_old |&gt;\n  broom::tidy()\nridge_coef_old\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.00489   0.168\n2 BA           0.0534    0.168\n3 PA          -0.194     0.168\n4 PC_score    -0.203     0.168\n5 OPS         -0.0711    0.168\n6 SORate       0.0768    0.168\n7 age         -0.502     0.168\n\n\nThe ridge regression model has been completed below.\n\npredictions_ridge_old &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_old |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 622 × 31\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adria…         0.244  0.801  2000 top            575 0.29     20    21 1   e6\n 2 Al Ma…         0.466 -0.239  1996 bottom         694 0.3      18    28 5   e5\n 3 Al Ne…         1.38   0.810  1988 lower          295 0.223     0    28 1   e5\n 4 Alan …        -0.439 -0.389  1986 upper          653 0.277    21    28 5.67e5\n 5 Alber…        -0.620 -0.230  1994 bottom         480 0.357    36    28 2.78e6\n 6 Alex …         1.54   0.733  1991 bottom         452 0.295     0    26 1.55e5\n 7 Alex …         1.07   0.420  1994 lower          398 0.296     4    29 3.75e5\n 8 Alex …         1.51   1.35   1995 middle         425 0.243    10    22 1.3 e5\n 9 Alex …         0.583  0.932  1997 middle         478 0.239    12    24 5   e5\n10 Alex …         1.44   0.855  1999 bottom         591 0.277    14    22 2.01e5\n# ℹ 612 more rows\n# ℹ 21 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;, Salary_new &lt;dbl&gt;\n\nrmse(predictions_ridge_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.688\n\n\n\nridge_model_new &lt;- linear_reg(mode = \"regression\", engine = \"glmnet\",\n                          penalty = tune(), # let's tune the lambda penalty term\n                          mixture = 0) # mixture = 0 specifies pure ridge regression\n\nridge_wflow_new &lt;- workflow() |&gt;\n  add_model(ridge_model_new)\n\nThe same process is repeated for the ‘new’ model. First with the recipe.\n\nridge_recipe_new &lt;- recipe(\n  Salary_ZScore ~ BA + PA + PC_score + OPS + SORate + age, # response ~ predictors\n  data = train_new\n) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; # don't scale the response\n  step_dummy(all_nominal_predictors())\n\nridge_wflow_new &lt;- ridge_wflow_new |&gt;\n  add_recipe(ridge_recipe_new)\n\nThen, the new model is tuned and cross validated.\n\nset.seed(1332)\nbatting_cv_new &lt;- vfold_cv(train_new, v = 10)\n\nridge_tune_new &lt;- tune_grid(ridge_model_new, \n                         ridge_recipe_new,\n                      resamples = batting_cv_new, \n                      grid = grid_regular(penalty(range = c(-1, 2.5)), levels = 50))\n\nThe metrics are collected.\n\nridge_tune_new |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(mapping = aes(x = penalty, y = mean)) + geom_point() + geom_line() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nThe workflow is now updated with the new ‘rmse’ value.\n\nridge_best_new &lt;- ridge_tune_new |&gt;\n  select_by_one_std_err(\n    metric = \"rmse\",\n    desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest\n)\nridge_best_new\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.228 Preprocessor1_Model06\n\nridge_wflow_final_new &lt;- ridge_wflow_new |&gt;\n  finalize_workflow(parameters = ridge_best_new) \n\nridge_wflow_final_new &lt;- fit(ridge_wflow_final_new, data=train_new) \n\nThe predictions are made.\n\nridge_pred_check_new &lt;- ridge_wflow_final_new |&gt;\n  fit_resamples(\n    resamples = batting_cv_new,\n    # save the cross-validated predictions\n    control = control_resamples(save_pred = TRUE)\n) |&gt; \n  collect_predictions()\n\n# using built-in defaults from probably\nlibrary(ggplot2)\n\n# Assume y_actual and y_pred are your actual and predicted values\ndf &lt;- data.frame(test_new, y_pred = predict(ridge_wflow_final_new, new_data = test_new))\n\nggplot(df, aes(x = .pred, y = Salary_ZScore)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Calibration Plot\", x = \"Predicted\", y = \"Actual\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMy calibration plot shows that the model follows a cubic distribution and it is predicting values that fit the curve and is more or less reliable in this regard.\nThe training data is fit.\n\nridge_fit_new &lt;- ridge_wflow_final_new |&gt;\n  fit(data = train_new)\nridge_fit_new\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_mean()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0) \n\n    Df  %Dev Lambda\n1    6  0.00 564.80\n2    6  0.25 514.60\n3    6  0.28 468.90\n4    6  0.30 427.20\n5    6  0.33 389.30\n6    6  0.37 354.70\n7    6  0.40 323.20\n8    6  0.44 294.50\n9    6  0.48 268.30\n10   6  0.53 244.50\n11   6  0.58 222.80\n12   6  0.64 203.00\n13   6  0.70 184.90\n14   6  0.77 168.50\n15   6  0.84 153.50\n16   6  0.92 139.90\n17   6  1.01 127.50\n18   6  1.10 116.10\n19   6  1.21 105.80\n20   6  1.32  96.43\n21   6  1.45  87.86\n22   6  1.58  80.06\n23   6  1.73  72.94\n24   6  1.89  66.46\n25   6  2.07  60.56\n26   6  2.26  55.18\n27   6  2.47  50.28\n28   6  2.70  45.81\n29   6  2.95  41.74\n30   6  3.22  38.03\n31   6  3.51  34.65\n32   6  3.82  31.58\n33   6  4.16  28.77\n34   6  4.53  26.21\n35   6  4.93  23.89\n36   6  5.36  21.76\n37   6  5.82  19.83\n38   6  6.32  18.07\n39   6  6.85  16.46\n40   6  7.41  15.00\n41   6  8.02  13.67\n42   6  8.67  12.45\n43   6  9.35  11.35\n44   6 10.08  10.34\n45   6 10.85   9.42\n46   6 11.67   8.58\n\n...\nand 54 more lines.\n\n\nThe lambda values are plotted.\n\nridge_fit_new |&gt;\n  extract_fit_engine() |&gt;\n  plot(xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nThe updated co-efficients are added.\n\nridge_coef_new &lt;- ridge_fit_new |&gt;\n  broom::tidy()\nridge_coef_new\n\n# A tibble: 7 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) -0.00876   0.228\n2 BA           0.0431    0.228\n3 PA          -0.176     0.228\n4 PC_score    -0.173     0.228\n5 OPS         -0.0593    0.228\n6 SORate       0.0556    0.228\n7 age         -0.477     0.228\n\n\nThe predictions are made.\n\npredictions_ridge_new &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 31\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…        1.27    0.383   2003 lower          533 0.312    11    26 3.65e5\n 2 A. J…       -0.617   0.0363  2006 upper          543 0.295    16    29 4   e6\n 3 A. J…        0.0190 -0.394   2011 upper          500 0.287     8    34 2   e6\n 4 Aaro…        1.31    0.254   2007 middle         657 0.291    17    25 3.95e5\n 5 Aaro…       -0.467  -0.0402  2010 middle         580 0.205    26    28 4   e6\n 6 Aaro…        1.36    0.320   2004 lower          566 0.293     6    27 3   e5\n 7 Aaro…        1.29    0.322   2006 upper          471 0.263     2    29 3.5 e5\n 8 Aaro…        0.511   0.249   2007 upper          449 0.29      2    30 1   e6\n 9 Aaro…        0.211   0.191   2008 upper          408 0.317     4    31 1.4 e6\n10 Aaro…        1.24    0.133   2004 upper          534 0.31     24    27 3.4 e5\n# ℹ 718 more rows\n# ℹ 21 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;, Salary_new &lt;dbl&gt;"
  },
  {
    "objectID": "Baseball.html#insights",
    "href": "Baseball.html#insights",
    "title": "Baseball",
    "section": "Insights",
    "text": "Insights\nI saw that the model did hint at a possibility that there was a marginal difference even though we cannot reject the null hypothesis. That being said, there are a few takeaways from this experiment that I gathered. One is that the distributions of the batting statistics across the board for both era’s are very similar at each percentile range. The players are very consistently producing a very similar distribution of HR, BA, etc year after year. Also, the salaries are very skewed for the league best players and the best players as expected would receive the highest pay.\n\nLimitations and Future Work\nThe clear limitation to this project would be the overall limited-ness of the salary database. As mentioned previously, there was not a very large data set for salaries since they only went back to the 1980s. There was also no clear distinction of the salary cap of each team during this time and no mentions of a player getting payed more because he played for the Yankees for example. Future work could include just looking at the top few teams in the game to separate this dataset even more to account for the fact that the most valuable teams will pay their players the most money.\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "Baseball.html#data-limitations",
    "href": "Baseball.html#data-limitations",
    "title": "Baseball",
    "section": "Data Limitations",
    "text": "Data Limitations\nDue to the size of the Salary database, I was only able to look at the years from 1985-2016. All year prior to 1985 were omitted and I was limited with the range of year that I was given."
  },
  {
    "objectID": "Baseball.html#experiment",
    "href": "Baseball.html#experiment",
    "title": "Baseball",
    "section": "Experiment",
    "text": "Experiment\nIn this experiment, the test set from the ‘new’ era will be placed into the ‘old’ model and vice versa to see, which model predicts larger salary z_score values.\npredictions_ridge_old_new will be the ‘old model’ with ‘new data’ predictions_ridge_new will be the ‘new model’ with the ‘new data’\n\npredictions_ridge_old_new &lt;- ridge_fit_old |&gt;\n  broom::augment(new_data = test_new)\npredictions_ridge_old_new |&gt;\n  dplyr::select(Name,\n                Salary_ZScore,\n    \n    everything()\n)\n\n# A tibble: 728 × 31\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…        1.27    0.350   2003 lower          533 0.312    11    26 3.65e5\n 2 A. J…       -0.617  -0.0116  2006 upper          543 0.295    16    29 4   e6\n 3 A. J…        0.0190 -0.486   2011 upper          500 0.287     8    34 2   e6\n 4 Aaro…        1.31    0.217   2007 middle         657 0.291    17    25 3.95e5\n 5 Aaro…       -0.467  -0.112   2010 middle         580 0.205    26    28 4   e6\n 6 Aaro…        1.36    0.285   2004 lower          566 0.293     6    27 3   e5\n 7 Aaro…        1.29    0.297   2006 upper          471 0.263     2    29 3.5 e5\n 8 Aaro…        0.511   0.222   2007 upper          449 0.29      2    30 1   e6\n 9 Aaro…        0.211   0.165   2008 upper          408 0.317     4    31 1.4 e6\n10 Aaro…        1.24    0.0947  2004 upper          534 0.31     24    27 3.4 e5\n# ℹ 718 more rows\n# ℹ 21 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;, Salary_new &lt;dbl&gt;\n\npredictions_ridge_old_new |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 728 × 31\n   Name  Salary_ZScore   .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 A. J…        1.27    0.350   2003 lower          533 0.312    11    26 3.65e5\n 2 A. J…       -0.617  -0.0116  2006 upper          543 0.295    16    29 4   e6\n 3 A. J…        0.0190 -0.486   2011 upper          500 0.287     8    34 2   e6\n 4 Aaro…        1.31    0.217   2007 middle         657 0.291    17    25 3.95e5\n 5 Aaro…       -0.467  -0.112   2010 middle         580 0.205    26    28 4   e6\n 6 Aaro…        1.36    0.285   2004 lower          566 0.293     6    27 3   e5\n 7 Aaro…        1.29    0.297   2006 upper          471 0.263     2    29 3.5 e5\n 8 Aaro…        0.511   0.222   2007 upper          449 0.29      2    30 1   e6\n 9 Aaro…        0.211   0.165   2008 upper          408 0.317     4    31 1.4 e6\n10 Aaro…        1.24    0.0947  2004 upper          534 0.31     24    27 3.4 e5\n# ℹ 718 more rows\n# ℹ 21 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;, Salary_new &lt;dbl&gt;\n\nrmse(predictions_ridge_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.723\n\nrmse(predictions_ridge_old_new, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.719\n\n\npredictions_ridge_new_old will be the ‘new model’ with the ‘old data’ predictions_ridge_old will be the ‘old model’ with the ‘old data’\n\npredictions_ridge_new_old &lt;- ridge_fit_new |&gt;\n  broom::augment(new_data = test_old)\npredictions_ridge_new_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 622 × 31\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adria…         0.244  0.805  2000 top            575 0.29     20    21 1   e6\n 2 Al Ma…         0.466 -0.172  1996 bottom         694 0.3      18    28 5   e5\n 3 Al Ne…         1.38   0.783  1988 lower          295 0.223     0    28 1   e5\n 4 Alan …        -0.439 -0.272  1986 upper          653 0.277    21    28 5.67e5\n 5 Alber…        -0.620 -0.150  1994 bottom         480 0.357    36    28 2.78e6\n 6 Alex …         1.54   0.719  1991 bottom         452 0.295     0    26 1.55e5\n 7 Alex …         1.07   0.412  1994 lower          398 0.296     4    29 3.75e5\n 8 Alex …         1.51   1.24   1995 middle         425 0.243    10    22 1.3 e5\n 9 Alex …         0.583  0.883  1997 middle         478 0.239    12    24 5   e5\n10 Alex …         1.44   0.826  1999 bottom         591 0.277    14    22 2.01e5\n# ℹ 612 more rows\n# ℹ 21 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;, Salary_new &lt;dbl&gt;\n\npredictions_ridge_old |&gt;\n  dplyr::select(\n    Name,\n    Salary_ZScore, \n    .pred,\n    everything()\n)\n\n# A tibble: 622 × 31\n   Name   Salary_ZScore  .pred  Year team_payroll    PA    BA    HR   age salary\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 Adria…         0.244  0.801  2000 top            575 0.29     20    21 1   e6\n 2 Al Ma…         0.466 -0.239  1996 bottom         694 0.3      18    28 5   e5\n 3 Al Ne…         1.38   0.810  1988 lower          295 0.223     0    28 1   e5\n 4 Alan …        -0.439 -0.389  1986 upper          653 0.277    21    28 5.67e5\n 5 Alber…        -0.620 -0.230  1994 bottom         480 0.357    36    28 2.78e6\n 6 Alex …         1.54   0.733  1991 bottom         452 0.295     0    26 1.55e5\n 7 Alex …         1.07   0.420  1994 lower          398 0.296     4    29 3.75e5\n 8 Alex …         1.51   1.35   1995 middle         425 0.243    10    22 1.3 e5\n 9 Alex …         0.583  0.932  1997 middle         478 0.239    12    24 5   e5\n10 Alex …         1.44   0.855  1999 bottom         591 0.277    14    22 2.01e5\n# ℹ 612 more rows\n# ℹ 21 more variables: R &lt;int&gt;, RBI &lt;int&gt;, SlugPct &lt;dbl&gt;, OBP &lt;dbl&gt;,\n#   BBRate &lt;dbl&gt;, SORate &lt;dbl&gt;, DoubleRate &lt;dbl&gt;, TripleRate &lt;dbl&gt;,\n#   HRRate &lt;dbl&gt;, RRate &lt;dbl&gt;, RBIRate &lt;dbl&gt;, SBRate &lt;dbl&gt;,\n#   SBSuccessRate &lt;dbl&gt;, PC_score &lt;dbl&gt;, PC1 &lt;dbl&gt;, PC2 &lt;dbl&gt;, PC3 &lt;dbl&gt;,\n#   OPS &lt;dbl&gt;, bats &lt;fct&gt;, lgID &lt;fct&gt;, Salary_new &lt;dbl&gt;\n\nrmse(predictions_ridge_new_old, truth = Salary_ZScore, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.693\n\n\nAs it turns out, there is a difference between the models and the older model predicts higher salary z-scores for both models indicating that the older players are predicted to be payed more than the new players thus showing that there is a pay disparity between the older group of players during the steroid era and the newer group of players.\nNow, I will check for the significance of this disparity to see if any conclusions can be drawn from this model by conducting a two-sample t-test.\nFirst it will judge based on the ‘new data’\n\ngroupA&lt;-predictions_ridge_old_new$.pred\ngroupB&lt;-predictions_ridge_new$.pred\n\n\nt.test(groupA, groupB, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  groupA and groupB\nt = -1.1569, df = 1440.3, p-value = 0.2475\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.10019797  0.02585487\nsample estimates:\n  mean of x   mean of y \n-0.01244027  0.02473128 \n\n\nThen, it will judge based on the ‘old data’\n\ngroup1&lt;-predictions_ridge_old$.pred\ngroup2&lt;-predictions_ridge_new_old$.pred\n\nt.test(group1, group2, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = -1.3815, df = 1231.2, p-value = 0.1674\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.12042779  0.02090694\nsample estimates:\n  mean of x   mean of y \n-0.03884412  0.01091631 \n\n\nUpon further investigation, the two sample t.test shows that there is not enough evidence to support the alternative hypothesis that there is a difference of means and thus we conclude not only that the players in the old data set and the new data set have no significant difference in stats but adjusting for inflation no significant differences in salaries as well. With p-values of .13 and .09, we can conclude that there is most likely a mild correlation between old players and higher salaries but not anything conclusive enough to overturn the null hypothesis."
  },
  {
    "objectID": "Baseball.html#motivation-or-context",
    "href": "Baseball.html#motivation-or-context",
    "title": "Baseball",
    "section": "",
    "text": "Viewing baseball in recent years has produced so many incredible moments that I am naturally drawn to use baseball as subject matter for this project. I have watched the sport for a very long time, and my primary piece of curiosity comes from an extension of the HR. As it is known to any person remotely involved in baseball, the use of illegal performance enhancing anabolic steroids to make players stronger so that they can hit the ball further has been a part of the game for over 100 years when Babe Ruth used steroids derived from sheep to boost his home run numbers."
  },
  {
    "objectID": "Baseball.html#main-question",
    "href": "Baseball.html#main-question",
    "title": "Baseball",
    "section": "Main Question",
    "text": "Main Question\nThe sport has definitely had influxes throughout its existence and there have been more egregious examples of cheating than others, particularly Barry Bonds. This led me to wonder, do salaries go up when these influxes of steroids go up? This would make sense since hr’s sell tickets and perhaps some of the most astronomical HR numbers have occurred during this point. So I wanted to compare the salaries of two eras (1985-2000) and (2001-2016) in order to understand if there were any era’s that were being paid more relative to the league average of previous years. This would prove difficult since this would have to factor inflation, but my main quest is to see if there were any salary changes during two 15 year intervals around the time where steroids spiked.\nPackages Used\n\nlibrary(Lahman)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(ggplot2)\n\nTidyverse/Tidy models - These packages provided easier and simpler code for statistical modeling project due to their build-in functions specifically built in for specific functions that are simplified for the user such as cross-validation, knn, ridge regression, etc.\nCorrplot - specific functions that create a correlation matrix and visually represent it\nGgplot2 - creates plots that I will use to see the calibration of my model\nData Used\nThe Lahman dataset is a data set created by Sean Lahman, which is the most comprehensive data set ever assembled with baseball stats going back over 100 years. In this data set, every season from almost every contributing pitcher and batter were recorded to show the tallies for the seasons that these players had. These include the full list of counting stats for players such as ‘At Bats’, ‘Strikeouts’, ‘Team’, ‘Age’, ‘RBI’, ‘BB’, ‘IBB’, ‘Hits’, etc. Due to the large size of the data set as well as the myriad of different baseball metrics that are used, it remains as one of the largest and most comprehensive baseball data sets available online today."
  }
]